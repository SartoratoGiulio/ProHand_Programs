{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8502,
     "status": "ok",
     "timestamp": 1693989173103,
     "user": {
      "displayName": "Giulio Sartorato",
      "userId": "16312762790553065985"
     },
     "user_tz": -120
    },
    "id": "1XWGcasCx4_l",
    "outputId": "93b0e7ab-76a3-4887-d120-83e03dd6382d"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "\n",
    "import argparse\n",
    "import csv\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from scipy.stats import loguniform\n",
    "from scipy.io import loadmat\n",
    "from sklearn import metrics\n",
    "from torch.utils import data\n",
    "from IPython.display import clear_output\n",
    "\n",
    "#sys.path.append('/content/drive/MyDrive/Tesi_Magistrale')\n",
    "sys.path.append('/content/drive/MyDrive/Tesi_Magistrale')\n",
    "sys.path.append('D:/Tesi_Magistrale')\n",
    "from Data_Processing_Utils import Norm_each_sub_by_own_param,NormalizeData, train_val_test_split_df, PlotLoss, plot_confusion_matrix, \\\n",
    "    Get_Sub_Norm_params, windowing_Dataframe\n",
    "\n",
    "from DATALOADERS import dataframe_dataset_triplet, Pandas_Dataset\n",
    "\n",
    "from MODELS import MKCNN, MKCNN_grid, random_choice, train_model_triplet, pre_train_model_triplet, train_model_standard, MultiKernelConv2D_grid\n",
    "\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 272,
     "status": "ok",
     "timestamp": 1693989188964,
     "user": {
      "displayName": "Giulio Sartorato",
      "userId": "16312762790553065985"
     },
     "user_tz": -120
    },
    "id": "KaI6IAwJyClS",
    "outputId": "26c99e85-234e-4705-8c45-ee6065245dac"
   },
   "outputs": [],
   "source": [
    "#file_path = '/content/drive/MyDrive/Tesi_Magistrale/'\n",
    "#data_path = '/content/drive/MyDrive/Tesi_Magistrale/DATA/'\n",
    "\n",
    "file_path = 'D:/Tesi_Magistrale'\n",
    "data_path = 'D:/Tesi_Magistrale/DATA/'\n",
    "\n",
    "sub_dir_list = [x for x in os.listdir(data_path) if 'offset_relabel' in x]\n",
    "sub_days_dir_list = []\n",
    "sub_day_data_dir_list = []\n",
    "current_day = []\n",
    "\n",
    "k = 0\n",
    "print(\"Datasets found:\")\n",
    "for i in range(len(sub_dir_list)):\n",
    "    print(f\"{i+1}. {sub_dir_list[i]}\")\n",
    "    sub_days_dir_list.append([x for x in os.listdir(data_path+sub_dir_list[i]+'/') if 'day' in x and not 'merged' in x])\n",
    "    for j in range(len(sub_days_dir_list[i])):\n",
    "      print(f\"\\t{i+1}.{j+1}. {sub_days_dir_list[i][j]}\")\n",
    "      current_day_list = [x for x in os.listdir(data_path+sub_dir_list[i]+'/'+sub_days_dir_list[i][j]+'/csv/') if '6rep' in x]\n",
    "      current_day.append(current_day_list)\n",
    "      for f in current_day_list:\n",
    "        print(f\"\\t\\t{i+1}.{j+1}.{k+1}. {f}\")\n",
    "        k+=1\n",
    "      k=0\n",
    "    sub_day_data_dir_list.append(current_day)\n",
    "    current_day = []\n",
    "    j=0\n",
    "\n",
    "del i\n",
    "del j\n",
    "del k\n",
    "del current_day\n",
    "del current_day_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sub_day_data_dir_list[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'ntpath' has no attribute 'exist'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10080\\1158390899.py\u001b[0m in \u001b[0;36m<cell line: 62>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;32mdel\u001b[0m \u001b[0mangle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[0mclear_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_path\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34mf'Multi_Sub_dataset/day{day}/Dataframe/multi_sub_multi_day_wnd_200_zero_to_one.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m     \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_path\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34mf'Multi_Sub_dataset/day{day}/Dataframe/multi_sub_multi_day_wnd_200_zero_to_one.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_path\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34mf'Multi_Sub_dataset/day{day}/Dataframe/multi_sub_multi_day_wnd_200_zero_to_one.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'ntpath' has no attribute 'exist'"
     ]
    }
   ],
   "source": [
    "dfs = []\n",
    "prev_df_lenghts = 0\n",
    "i = 0\n",
    "day = 1\n",
    "for subj in range(1,len(sub_day_data_dir_list)+1):\n",
    "    for angle in range(1,len(sub_day_data_dir_list[subj-1][day-1])+1):\n",
    "        current_df_path = data_path + sub_dir_list[subj-1]+'/'+sub_days_dir_list[subj-1][day-1]+'/csv/' + [ x for x in sub_day_data_dir_list[subj-1][day-1] if f'angle_{angle}' in x][0]\n",
    "        print(f\"Loading Sub {subj}, day {day}, angle {angle}\")\n",
    "        df = pd.read_csv(current_df_path)\n",
    "        # prepare dataset for windowing\n",
    "        # sub 2 acquisition are on left hand. Swap channels to compare it with right hands\n",
    "        if subj == 2:\n",
    "            df.rename(columns = {'Channel_1' : 'Ch8',\n",
    "                'Channel_2' : 'Ch7',\n",
    "                'Channel_3' : 'Ch6',\n",
    "                'Channel_4' : 'Ch5',\n",
    "                'Channel_5' : 'Ch4',\n",
    "                'Channel_6' : 'Ch3',\n",
    "                'Channel_7' : 'Ch2',\n",
    "                'Channel_8' : 'Ch1',\n",
    "                'Relabelled_repetition' : 'Rep_Num',\n",
    "                'Relabelled_stimulus' : 'Exe_Num',\n",
    "                }, inplace = True)\n",
    "        else:\n",
    "            df.rename(columns = {'Channel_1' : 'Ch1',\n",
    "                'Channel_2' : 'Ch2',\n",
    "                'Channel_3' : 'Ch3',\n",
    "                'Channel_4' : 'Ch4',\n",
    "                'Channel_5' : 'Ch5',\n",
    "                'Channel_6' : 'Ch6',\n",
    "                'Channel_7' : 'Ch7',\n",
    "                'Channel_8' : 'Ch8',\n",
    "                'Relabelled_repetition' : 'Rep_Num',\n",
    "                'Relabelled_stimulus' : 'Exe_Num',\n",
    "                }, inplace = True)\n",
    "        df = df.loc[df['Exe_Num'] != 0]\n",
    "        df['Exe_Num'] = df['Exe_Num']-1\n",
    "        df['Sub'] = [subj]*len(df)\n",
    "        df.drop(['Repetition', 'Stimulus'], axis = 1, inplace = True)\n",
    "        df = windowing_Dataframe(df, 200, overlap_perc=0.5, NinaPro_Database_Num=None, drop_last=False, keep_angle = True, savefile = False)\n",
    "        df.reset_index(inplace = True)\n",
    "        df['sample_index'] = df['sample_index'] + prev_df_lenghts + i\n",
    "        df['angle_index'] = [angle]*len(df)\n",
    "        df['day'] = [day]*len(df)\n",
    "        cols = [f'ch{c}' for c in range(1,9)]\n",
    "        df[cols] = df[cols].astype(np.uint16)\n",
    "        df['angle'] = df['angle'].astype(np.int16)\n",
    "        \n",
    "        dfs.append(df)\n",
    "        prev_df_lenghts = df['sample_index'].iloc[-1]\n",
    "        i=1\n",
    "    \n",
    "#concatenate all single dfs\n",
    "del prev_df_lenghts\n",
    "del i\n",
    "df = pd.concat(dfs)\n",
    "del dfs\n",
    "del subj\n",
    "del day\n",
    "del angle\n",
    "clear_output()\n",
    "if not os.path.exists(data_path + f'Multi_Sub_dataset/day{day}/Dataframe/'):\n",
    "    os.makedirs(data_path + f'Multi_Sub_dataset/day{day}/Dataframe/')\n",
    "df.to_csv(data_path + f'Multi_Sub_dataset/day{day}/Dataframe/multi_sub_multi_day_wnd_200.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1], dtype=int64)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(df['day'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 519
    },
    "executionInfo": {
     "elapsed": 10046,
     "status": "error",
     "timestamp": 1693989152681,
     "user": {
      "displayName": "Giulio Sartorato",
      "userId": "16312762790553065985"
     },
     "user_tz": -120
    },
    "id": "ZvMA0bU_VndF",
    "jupyter": {
     "source_hidden": true
    },
    "outputId": "e9b87360-26e2-4ce2-ff01-f977347032cd"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "dfs = []\n",
    "prev_df_lenghts = 0\n",
    "i = 0\n",
    "for subj in range(1,len(sub_day_data_dir_list)+1):\n",
    "    for day in range(1,len(sub_day_data_dir_list[subj-1])+1):\n",
    "        for angle in range(1,len(sub_day_data_dir_list[subj-1][day-1])+1):\n",
    "            current_df_path = data_path + sub_dir_list[subj-1]+'/'+sub_days_dir_list[subj-1][day-1]+'/csv/' + [ x for x in sub_day_data_dir_list[subj-1][day-1] if f'angle_{angle}' in x][0]\n",
    "            print(f\"Loading Sub {subj}, day {day}, angle {angle}\")\n",
    "            df = pd.read_csv(current_df_path)\n",
    "            \n",
    "            # prepare dataset for windowing\n",
    "            # sub 2 acquisition are on left hand. Swap channels to compare it with right hands\n",
    "            if subj == 2:\n",
    "                df.rename(columns = {'Channel_1' : 'Ch8',\n",
    "                    'Channel_2' : 'Ch7',\n",
    "                    'Channel_3' : 'Ch6',\n",
    "                    'Channel_4' : 'Ch5',\n",
    "                    'Channel_5' : 'Ch4',\n",
    "                    'Channel_6' : 'Ch3',\n",
    "                    'Channel_7' : 'Ch2',\n",
    "                    'Channel_8' : 'Ch1',\n",
    "                    'Relabelled_repetition' : 'Rep_Num',\n",
    "                    'Relabelled_stimulus' : 'Exe_Num',\n",
    "                    }, inplace = True)\n",
    "            else:\n",
    "                df.rename(columns = {'Channel_1' : 'Ch1',\n",
    "                    'Channel_2' : 'Ch2',\n",
    "                    'Channel_3' : 'Ch3',\n",
    "                    'Channel_4' : 'Ch4',\n",
    "                    'Channel_5' : 'Ch5',\n",
    "                    'Channel_6' : 'Ch6',\n",
    "                    'Channel_7' : 'Ch7',\n",
    "                    'Channel_8' : 'Ch8',\n",
    "                    'Relabelled_repetition' : 'Rep_Num',\n",
    "                    'Relabelled_stimulus' : 'Exe_Num',\n",
    "                    }, inplace = True)\n",
    "            df = df.loc[df['Exe_Num'] != 0]\n",
    "            df['Exe_Num'] = df['Exe_Num']-1\n",
    "            df['Sub'] = [subj]*len(df)\n",
    "            df.drop(['Repetition', 'Stimulus'], axis = 1, inplace = True)\n",
    "            df = windowing_Dataframe(df, 200, overlap_perc=0.5, NinaPro_Database_Num=None, drop_last=False, keep_angle = True, savefile = False)\n",
    "            df.reset_index(inplace = True)\n",
    "            df['sample_index'] = df['sample_index'] + prev_df_lenghts + i\n",
    "            df['angle_index'] = [angle]*len(df)\n",
    "            df['day'] = [day]*len(df)\n",
    "            cols = [f'ch{c}' for c in range(1,9)]\n",
    "            df[cols] = df[cols].astype(np.uint16)\n",
    "            df['angle'] = df['angle'].astype(np.int16)\n",
    "            \n",
    "            dfs.append(df)\n",
    "            prev_df_lenghts = df['sample_index'].iloc[-1]\n",
    "            i=1\n",
    "        \n",
    "#concatenate all single dfs\n",
    "del prev_df_lenghts\n",
    "del i\n",
    "df = pd.concat(dfs)\n",
    "del dfs\n",
    "del subj\n",
    "del day\n",
    "del angle\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9443,
     "status": "ok",
     "timestamp": 1693987174684,
     "user": {
      "displayName": "Giulio Sartorato",
      "userId": "16312762790553065985"
     },
     "user_tz": -120
    },
    "id": "SQ6WXNM_6PxP",
    "outputId": "6d7b2c5a-64d2-4dc1-ec14-ad1c548205be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing Sub 1, day 1\n",
      "Normalizing Sub 2, day 1\n",
      "Normalizing Sub 3, day 1\n"
     ]
    }
   ],
   "source": [
    "# Normalize\n",
    "\n",
    "NORM_MODE = 'sub'\n",
    "NORM_TYPE = 'zero_to_one'\n",
    "RECT = True\n",
    "\n",
    "n_sub = np.unique(df['sub'])\n",
    "n_day = np.unique(df['day'])\n",
    "dfs = []\n",
    "df_idx = df.set_index(['sub','day'])\n",
    "\n",
    "for sub in n_sub:\n",
    "  for day in n_day:\n",
    "    try:\n",
    "      current_df = df_idx.loc[sub, day]\n",
    "      print(f\"Normalizing Sub {sub}, day {day}\")\n",
    "      chs = current_df[cols]\n",
    "      M = chs.values.max()\n",
    "      m = chs.values.min()\n",
    "      chs = (chs-m)/(M-m)\n",
    "      current_df[cols] = chs[cols]\n",
    "      dfs.append(current_df.reset_index())\n",
    "    except KeyError:\n",
    "      pass\n",
    "df = pd.concat(dfs, axis = 0)\n",
    "del dfs\n",
    "del M\n",
    "del m\n",
    "del chs\n",
    "del current_df\n",
    "del sub\n",
    "del day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "executionInfo": {
     "elapsed": 283678,
     "status": "ok",
     "timestamp": 1693987556543,
     "user": {
      "displayName": "Giulio Sartorato",
      "userId": "16312762790553065985"
     },
     "user_tz": -120
    },
    "id": "AkoDbXjke3Tv"
   },
   "outputs": [],
   "source": [
    "df.to_csv(data_path + 'Multi_Sub_dataset/multi_sub_multi_day_wnd_200_zero_to_one.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 72276,
     "status": "ok",
     "timestamp": 1693989284778,
     "user": {
      "displayName": "Giulio Sartorato",
      "userId": "16312762790553065985"
     },
     "user_tz": -120
    },
    "id": "yiqwJHTSiA6_",
    "outputId": "4e3c9db7-92ec-42f7-8a8d-49d7ff7fe4c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single Model: True\n",
      "Test Rep: 3\n",
      "Dataset Loaded\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "single_model = True\n",
    "data_path = '/content/drive/MyDrive/Tesi_Magistrale/DATA/'\n",
    "NORM_MODE = 'sub'\n",
    "NORM_TYPE = 'zero_to_one'\n",
    "RECT = True\n",
    "train_val_test_rep = [[2, 4, 6], [1, 5], [3]]\n",
    "test_rep = train_val_test_rep[2][0]\n",
    "WND_LEN = 200\n",
    "print(f\"Single Model: {single_model}\\nTest Rep: {test_rep}\")\n",
    "exe_labels = ['medium_wrap', 'lateral', 'power_sphere', 'power_disk', 'prismatic_pinch', 'index_extension','wave_out', 'wave_in', 'fist', 'open_hand']\n",
    "df = pd.read_csv(data_path + f\"/Multi_Sub_dataset/multi_sub_multi_day_wnd_{WND_LEN}_zero_to_one.csv\")\n",
    "print(\"Dataset Loaded\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 920
    },
    "executionInfo": {
     "elapsed": 19027,
     "status": "error",
     "timestamp": 1693989356503,
     "user": {
      "displayName": "Giulio Sartorato",
      "userId": "16312762790553065985"
     },
     "user_tz": -120
    },
    "id": "KPkPM3cykD_3",
    "outputId": "dc92d497-d488-470a-cbdb-f6db735bb971"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-595caa60dd6d>:2: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df.loc[:, n_channels] = df.loc[:, n_channels].astype(np.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created with Num params: 108746\n",
      "###################################################################### \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Epoch 0/49 ---------------------------------------------------------------------- \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train -> loss: 2.24988, accuracy: 0.12500, kappa score: -0.0412, Epoch: -> 0 / 50  Batch Number: -> 1 / 1157\n",
      "train -> loss: 2.29627, accuracy: 0.07812, kappa score: -0.1637, Epoch: -> 0 / 50  Batch Number: -> 2 / 1157\n",
      "train -> loss: 2.31867, accuracy: 0.06250, kappa score: -0.1679, Epoch: -> 0 / 50  Batch Number: -> 3 / 1157\n",
      "train -> loss: 2.33008, accuracy: 0.07031, kappa score: -0.1063, Epoch: -> 0 / 50  Batch Number: -> 4 / 1157\n",
      "train -> loss: 2.31948, accuracy: 0.08750, kappa score: -0.0588, Epoch: -> 0 / 50  Batch Number: -> 5 / 1157\n",
      "train -> loss: 2.32041, accuracy: 0.07812, kappa score: -0.0791, Epoch: -> 0 / 50  Batch Number: -> 6 / 1157\n",
      "train -> loss: 2.33340, accuracy: 0.08036, kappa score: -0.0353, Epoch: -> 0 / 50  Batch Number: -> 7 / 1157\n",
      "train -> loss: 2.33271, accuracy: 0.08203, kappa score: -0.0425, Epoch: -> 0 / 50  Batch Number: -> 8 / 1157\n",
      "train -> loss: 2.32332, accuracy: 0.09028, kappa score: -0.0420, Epoch: -> 0 / 50  Batch Number: -> 9 / 1157\n",
      "train -> loss: 2.32373, accuracy: 0.08438, kappa score: -0.0650, Epoch: -> 0 / 50  Batch Number: -> 10 / 1157\n",
      "train -> loss: 2.31706, accuracy: 0.09091, kappa score: -0.0586, Epoch: -> 0 / 50  Batch Number: -> 11 / 1157\n",
      "train -> loss: 2.31284, accuracy: 0.09375, kappa score: -0.0549, Epoch: -> 0 / 50  Batch Number: -> 12 / 1157\n",
      "train -> loss: 2.31001, accuracy: 0.10337, kappa score: -0.0307, Epoch: -> 0 / 50  Batch Number: -> 13 / 1157\n",
      "train -> loss: 2.30832, accuracy: 0.10714, kappa score: -0.0046, Epoch: -> 0 / 50  Batch Number: -> 14 / 1157\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-595caa60dd6d>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Model created with Num params: {sum([p.numel() for p in model.parameters() if p.requires_grad])}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     best_weights, tr_losses, val_losses = train_model_standard(model=model, loss_fun=cross_entropy_loss,\n\u001b[0m\u001b[1;32m     65\u001b[0m                                                             \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                                                             dataloaders={\"train\": train_set_generator,\n",
      "\u001b[0;32m/content/drive/MyDrive/Tesi_Magistrale/MODELS.py\u001b[0m in \u001b[0;36mtrain_model_standard\u001b[0;34m(model, loss_fun, optimizer, dataloaders, scheduler, num_epochs, precision, patience, patience_increase, device)\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 537\u001b[0;31m                     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    538\u001b[0m                     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_channels = [cx for cx in df.columns if 'ch' in cx]\n",
    "df.loc[:, n_channels] = df.loc[:, n_channels].astype(np.float32)\n",
    "split_mode = 'rep'\n",
    "\n",
    "if single_model:\n",
    "    df_train, df_val, df_test = train_val_test_split_df(df, mode=split_mode, manual_sel=train_val_test_rep)\n",
    "    valid_group = np.unique(df_val[split_mode])\n",
    "    test_group = np.unique(df_test[split_mode])[0]\n",
    "    train_set = Pandas_Dataset(df_train.groupby('sample_index'))\n",
    "    valid_set = Pandas_Dataset(df_val.groupby('sample_index'))\n",
    "    test_set = Pandas_Dataset(df_test.groupby('sample_index'))\n",
    "    df_test.set_index('sample_index', inplace = True)\n",
    "\n",
    "    # %% Dataloaders\n",
    "    batch_size = 32\n",
    "    num_workers = 0\n",
    "    params = {'batch_size': batch_size,\n",
    "            'shuffle': True,\n",
    "            # 'sampler': sampler,\n",
    "            'num_workers': num_workers,\n",
    "            'drop_last': False}\n",
    "\n",
    "    train_set_generator = data.DataLoader(train_set, **params)\n",
    "    valid_set_generator = data.DataLoader(valid_set, **params)\n",
    "    test_set_generator = data.DataLoader(test_set, **params)\n",
    "\n",
    "    #%% Parametric model\n",
    "    kernels_gap = [g for g in range(0, 3 * round(WND_LEN / 20), round(WND_LEN / 20))]\n",
    "    kernel_sizes = np.full((3, 5, 2), [1, 3])\n",
    "    for j in range(3):\n",
    "        for i in range(5):\n",
    "            kernel_sizes[j][i][0] = (round(WND_LEN / 20) * (i + 1) + kernels_gap[j])\n",
    "\n",
    "    n_classes = 10\n",
    "\n",
    "    net = {'N_multik': 32, 'N_Conv_conc': 64, 'N_SepConv': 128,\n",
    "    'Kernel_multi_dim': kernel_sizes[0],\n",
    "    'Kernel_Conv_conc': 1,\n",
    "    'act_func': nn.ReLU(),\n",
    "    'Pool_Type': nn.MaxPool2d,\n",
    "    'wnd_len':WND_LEN\n",
    "    }\n",
    "\n",
    "    model = MKCNN_grid(net, num_classes=n_classes)\n",
    "    model = model.to(device)\n",
    "\n",
    "    #%% Loss Optim and scheduler\n",
    "    # Define Loss functions\n",
    "    cross_entropy_loss = nn.CrossEntropyLoss(reduction='mean').to(device)\n",
    "    # triplet = nn.TripletMarginLoss(reduction='mean', margin=1, p=2)\n",
    "\n",
    "    # Define Optimizer\n",
    "    learning_rate = 0.0001\n",
    "    # changed beta values from (0.5,0.999) to (0.9,0.999)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999), weight_decay=1e-4)\n",
    "\n",
    "    # # Define Scheduler\n",
    "    precision = 1e-6\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, factor=.2,\n",
    "                                                        patience=5, verbose=True, eps=precision)\n",
    "\n",
    "    print(f'Model created with Num params: {sum([p.numel() for p in model.parameters() if p.requires_grad])}')\n",
    "\n",
    "    best_weights, tr_losses, val_losses = train_model_standard(model=model, loss_fun=cross_entropy_loss,\n",
    "                                                            optimizer=optimizer, scheduler=scheduler,\n",
    "                                                            dataloaders={\"train\": train_set_generator,\n",
    "                                                                            \"val\": valid_set_generator},\n",
    "                                                            num_epochs=50, precision=precision,\n",
    "                                                            patience=10, patience_increase=5)\n",
    "\n",
    "    database = data_path + f\"/Multi_Sub_dataset/test_rep{test_rep}/\"\n",
    "    filename = 'multi_sub_multi_day_wnd_200_zero_to_one'\n",
    "    # Save state dict of the model\n",
    "    if not os.path.exists(database + f'Cross_{split_mode}/Best_States/'):\n",
    "        os.makedirs(database + f'Cross_{split_mode}/Best_States/')\n",
    "    torch.save(best_weights['state_dict'],\n",
    "            database + f'Cross_{split_mode}/Best_States/state_dict_{filename}.pth')\n",
    "\n",
    "    # %% PlotLoss\n",
    "    if not os.path.exists(database + f'Cross_{split_mode}/Plot/'):\n",
    "        os.makedirs(database + f'Cross_{split_mode}/Plot/')\n",
    "\n",
    "    PlotLoss(tr_losses, val_loss=val_losses,\n",
    "            title=f'Cross-{split_mode}',\n",
    "            path_to_save=database + f'Cross_{split_mode}/Plot/',\n",
    "            filename=f'Cross_{split_mode}_{filename}.png')\n",
    "\n",
    "    # %% Conf Matrix\n",
    "    if not os.path.exists(database + f'Cross_{split_mode}/Conf_Matrix/'):\n",
    "        os.makedirs(database + f'Cross_{split_mode}/Conf_Matrix/')\n",
    "\n",
    "    # Evaluation\n",
    "    softmax_block = nn.Softmax(dim=1)\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    test_dataloader = test_set_generator\n",
    "    net = model\n",
    "    # Load Weights\n",
    "    net.load_state_dict(torch.load(database + f'Cross_{split_mode}/Best_States/state_dict_{filename}.pth'))\n",
    "\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_dataloader:\n",
    "            # inputs = torch.swapaxes(inputs, 2, 1)  # -> convert from [10,20] to [20,10]\n",
    "            inputs = inputs[:, None, :, :]\n",
    "            inputs = inputs.to(device)\n",
    "            labels_np = labels.cpu().data.numpy()\n",
    "            # forward\n",
    "            outputs, _ = net(inputs)\n",
    "            outputs_np = softmax_block(outputs)\n",
    "            outputs_np = outputs_np.cpu().data.numpy()\n",
    "            outputs_np = np.argmax(outputs_np, axis=1)\n",
    "\n",
    "            y_pred = np.append(y_pred, outputs_np)\n",
    "            y_true = np.append(y_true, labels_np)\n",
    "\n",
    "        cm = metrics.confusion_matrix(y_true=y_true, y_pred=y_pred)\n",
    "        # Fancy confusion matrix\n",
    "        plot_confusion_matrix(cm, target_names=exe_labels, title=f'Confusion Matrix for {split_mode} {test_group}',\n",
    "                            path_to_save=database + f'Cross_{split_mode}/Conf_Matrix/Cross_{split_mode}_N_{filename}.png')\n",
    "\n",
    "    # %% Write in csv cross_sub results\n",
    "    # Building columns\n",
    "    header_net = [f'Tested_{split_mode}', 'Best Val Loss', 'Accuracy', 'Kappa', 'F1_score', 'Best Epoch', 'Norm_Type',\n",
    "                'Norm_Mode', 'Rect', f'Valid_{split_mode}']\n",
    "\n",
    "    # Open the CSV file and write the headers and row of values\n",
    "    with open(database + f'Cross_{split_mode}/Evals_single_model.csv', 'a', newline='') as myFile:\n",
    "        writer = csv.writer(myFile)\n",
    "        if myFile.tell() == 0:\n",
    "            writer.writerow(header_net)\n",
    "        # Create the row of values\n",
    "        row = [test_group, min(val_losses),\n",
    "            metrics.accuracy_score(y_true=y_true, y_pred=y_pred),\n",
    "            metrics.cohen_kappa_score(y1=y_true, y2=y_pred, weights='quadratic'),\n",
    "            metrics.f1_score(y_true=y_true, y_pred=y_pred, average='macro'),\n",
    "            best_weights['epoch'], NORM_TYPE, NORM_MODE, str(RECT), valid_group]\n",
    "        writer.writerow(row)\n",
    "        print(f'Results Saved in -> {database}/Cross_{split_mode}/Evals.csv')\n",
    "\n",
    "else:\n",
    "    for ANGLE in range(1,6):\n",
    "      current_df = df[df['angle_index'] == ANGLE]\n",
    "    df_train, df_val, df_test = train_val_test_split_df(df, mode=split_mode, manual_sel=train_val_test_rep)\n",
    "    valid_group = np.unique(df_val[split_mode])\n",
    "    test_group = np.unique(df_test[split_mode])[0]\n",
    "    train_set = Pandas_Dataset(df_train.groupby('sample_index'))\n",
    "    valid_set = Pandas_Dataset(df_val.groupby('sample_index'))\n",
    "    test_set = Pandas_Dataset(df_test.groupby('sample_index'))\n",
    "    df_test.set_index('sample_index', inplace = True)\n",
    "\n",
    "    # %% Dataloaders\n",
    "    batch_size = 32\n",
    "    num_workers = 0\n",
    "    params = {'batch_size': batch_size,\n",
    "            'shuffle': True,\n",
    "            # 'sampler': sampler,\n",
    "            'num_workers': num_workers,\n",
    "            'drop_last': False}\n",
    "\n",
    "    train_set_generator = data.DataLoader(train_set, **params)\n",
    "    valid_set_generator = data.DataLoader(valid_set, **params)\n",
    "    test_set_generator = data.DataLoader(test_set, **params)\n",
    "\n",
    "    #%% Parametric model\n",
    "    kernels_gap = [g for g in range(0, 3 * round(WND_LEN / 20), round(WND_LEN / 20))]\n",
    "    kernel_sizes = np.full((3, 5, 2), [1, 3])\n",
    "    for j in range(3):\n",
    "        for i in range(5):\n",
    "            kernel_sizes[j][i][0] = (round(WND_LEN / 20) * (i + 1) + kernels_gap[j])\n",
    "\n",
    "    n_classes = 10\n",
    "\n",
    "    net = {'N_multik': 32, 'N_Conv_conc': 64, 'N_SepConv': 128,\n",
    "    'Kernel_multi_dim': kernel_sizes[0],\n",
    "    'Kernel_Conv_conc': 1,\n",
    "    'act_func': nn.ReLU(),\n",
    "    'Pool_Type': nn.MaxPool2d,\n",
    "    'wnd_len':WND_LEN\n",
    "    }\n",
    "\n",
    "    model = MKCNN_grid(net, num_classes=n_classes)\n",
    "    model = model.to(device)\n",
    "\n",
    "    #%% Loss Optim and scheduler\n",
    "    # Define Loss functions\n",
    "    cross_entropy_loss = nn.CrossEntropyLoss(reduction='mean').to(device)\n",
    "    # triplet = nn.TripletMarginLoss(reduction='mean', margin=1, p=2)\n",
    "\n",
    "    # Define Optimizer\n",
    "    learning_rate = 0.0001\n",
    "    # changed beta values from (0.5,0.999) to (0.9,0.999)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999), weight_decay=1e-4)\n",
    "\n",
    "    # # Define Scheduler\n",
    "    precision = 1e-6\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, factor=.2,\n",
    "                                                        patience=5, verbose=True, eps=precision)\n",
    "\n",
    "    print(f'Model created with Num params: {sum([p.numel() for p in model.parameters() if p.requires_grad])}')\n",
    "\n",
    "    best_weights, tr_losses, val_losses = train_model_standard(model=model, loss_fun=cross_entropy_loss,\n",
    "                                                            optimizer=optimizer, scheduler=scheduler,\n",
    "                                                            dataloaders={\"train\": train_set_generator,\n",
    "                                                                            \"val\": valid_set_generator},\n",
    "                                                            num_epochs=50, precision=precision,\n",
    "                                                            patience=10, patience_increase=5)\n",
    "\n",
    "    database = data_path + f\"/Multi_Sub_dataset/test_rep{test_rep}/\"\n",
    "    filename = f'multi_sub_multi_day_wnd_200_zero_to_one_angle{ANGLE}'\n",
    "    # Save state dict of the model\n",
    "    if not os.path.exists(database + f'Cross_{split_mode}/Best_States/'):\n",
    "        os.makedirs(database + f'Cross_{split_mode}/Best_States/')\n",
    "    torch.save(best_weights['state_dict'],\n",
    "            database + f'Cross_{split_mode}/Best_States/state_dict_{filename}.pth')\n",
    "\n",
    "    # %% PlotLoss\n",
    "    if not os.path.exists(database + f'Cross_{split_mode}/Plot/'):\n",
    "        os.makedirs(database + f'Cross_{split_mode}/Plot/')\n",
    "\n",
    "    PlotLoss(tr_losses, val_loss=val_losses,\n",
    "            title=f'Cross-{split_mode}',\n",
    "            path_to_save=database + f'Cross_{split_mode}/Plot/',\n",
    "            filename=f'Cross_{split_mode}_{filename}.png')\n",
    "\n",
    "    # %% Conf Matrix\n",
    "    if not os.path.exists(database + f'Cross_{split_mode}/Conf_Matrix/'):\n",
    "        os.makedirs(database + f'Cross_{split_mode}/Conf_Matrix/')\n",
    "\n",
    "    # Evaluation\n",
    "    softmax_block = nn.Softmax(dim=1)\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    test_dataloader = test_set_generator\n",
    "    net = model\n",
    "    # Load Weights\n",
    "    net.load_state_dict(torch.load(database + f'Cross_{split_mode}/Best_States/state_dict_{filename}.pth'))\n",
    "\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_dataloader:\n",
    "            # inputs = torch.swapaxes(inputs, 2, 1)  # -> convert from [10,20] to [20,10]\n",
    "            inputs = inputs[:, None, :, :]\n",
    "            inputs = inputs.to(device)\n",
    "            labels_np = labels.cpu().data.numpy()\n",
    "            # forward\n",
    "            outputs, _ = net(inputs)\n",
    "            outputs_np = softmax_block(outputs)\n",
    "            outputs_np = outputs_np.cpu().data.numpy()\n",
    "            outputs_np = np.argmax(outputs_np, axis=1)\n",
    "\n",
    "            y_pred = np.append(y_pred, outputs_np)\n",
    "            y_true = np.append(y_true, labels_np)\n",
    "\n",
    "        cm = metrics.confusion_matrix(y_true=y_true, y_pred=y_pred)\n",
    "        # Fancy confusion matrix\n",
    "        plot_confusion_matrix(cm, target_names=exe_labels, title=f'Confusion Matrix for {split_mode} {test_group}',\n",
    "                            path_to_save=database + f'Cross_{split_mode}/Conf_Matrix/Cross_{split_mode}_N_{filename}.png')\n",
    "\n",
    "    # %% Write in csv cross_sub results\n",
    "    # Building columns\n",
    "    header_net = [f'Tested_{split_mode}', 'Best Val Loss', 'Accuracy', 'Kappa', 'F1_score', 'Best Epoch', 'Norm_Type',\n",
    "                'Norm_Mode', 'Rect', f'Valid_{split_mode}']\n",
    "\n",
    "    # Open the CSV file and write the headers and row of values\n",
    "    with open(database + f'Cross_{split_mode}/Evals_multi_model_angle_{ANGLE}.csv', 'a', newline='') as myFile:\n",
    "        writer = csv.writer(myFile)\n",
    "        if myFile.tell() == 0:\n",
    "            writer.writerow(header_net)\n",
    "        # Create the row of values\n",
    "        row = [test_group, min(val_losses),\n",
    "            metrics.accuracy_score(y_true=y_true, y_pred=y_pred),\n",
    "            metrics.cohen_kappa_score(y1=y_true, y2=y_pred, weights='quadratic'),\n",
    "            metrics.f1_score(y_true=y_true, y_pred=y_pred, average='macro'),\n",
    "            best_weights['epoch'], NORM_TYPE, NORM_MODE, str(RECT), valid_group]\n",
    "        writer.writerow(row)\n",
    "        print(f'Results Saved in -> {database}/Cross_{split_mode}/Evals.csv')\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNwwTBWPifU5nRAev8ha1Is",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
