{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9935,
     "status": "ok",
     "timestamp": 1694009648056,
     "user": {
      "displayName": "Giulio Sartorato",
      "userId": "16312762790553065985"
     },
     "user_tz": -120
    },
    "id": "XeEURP9Ni013",
    "outputId": "98995634-7ad0-4ab0-94ad-823d6b08f056"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "import argparse\n",
    "import csv\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from scipy.stats import loguniform\n",
    "from scipy.io import loadmat\n",
    "from sklearn import metrics\n",
    "from torch.utils import data\n",
    "from IPython.display import clear_output\n",
    "\n",
    "#sys.path.append('/content/drive/MyDrive/Tesi_Magistrale')\n",
    "sys.path.append('D:/Tesi_Magistrale')\n",
    "from Data_Processing_Utils import Norm_each_sub_by_own_param,NormalizeData, train_val_test_split_df, PlotLoss, plot_confusion_matrix, \\\n",
    "    Get_Sub_Norm_params, windowing_Dataframe\n",
    "\n",
    "from DATALOADERS import dataframe_dataset_triplet, Pandas_Dataset\n",
    "\n",
    "from MODELS import MKCNN, MKCNN_grid, random_choice, train_model_triplet, pre_train_model_triplet, train_model_standard, MultiKernelConv2D_grid\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 404,
     "status": "ok",
     "timestamp": 1694009651932,
     "user": {
      "displayName": "Giulio Sartorato",
      "userId": "16312762790553065985"
     },
     "user_tz": -120
    },
    "id": "2dK8sTABjMhC"
   },
   "outputs": [],
   "source": [
    "# custom pandas dataset class modified to provide also the angle\n",
    "class Pandas_Dataset_angle(data.Dataset):\n",
    "\n",
    "    def __init__(self, df_grouped_by_samples, return_sub=False):\n",
    "        self.grouped = df_grouped_by_samples\n",
    "        self.channels = [i for i in df_grouped_by_samples.obj.columns if 'ch' in i]\n",
    "        self.indices = list(df_grouped_by_samples.indices)\n",
    "        self.return_sub = return_sub\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.grouped)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        picked_smp = self.grouped.get_group(self.indices[index])\n",
    "        # Picking only the channels columns from a single sample\n",
    "        sample = torch.tensor(picked_smp.loc[:, self.channels].values).type(torch.float32)\n",
    "        # picking only one label for each sample\n",
    "        label = torch.tensor(picked_smp.loc[:, ['label']].head(1).values[0][0]).type(torch.int8)\n",
    "        angle = torch.tensor(picked_smp.loc[:, ['angle']].head(1).values[0][0]).type(torch.int16)\n",
    "        if self.return_sub:\n",
    "            # picking only one subject\n",
    "            sub = torch.tensor(picked_smp.loc[:, ['sub']].head(1).values[0][0]).type(torch.int8)\n",
    "            return sample, label, angle, sub\n",
    "        else:\n",
    "\n",
    "            # It's missing the part in which I Normalize the data for each subject, or for each subject and channels\n",
    "            # It's not a good idea to do that on the dataloader while producing results\n",
    "            return sample, label, angle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1694009653438,
     "user": {
      "displayName": "Giulio Sartorato",
      "userId": "16312762790553065985"
     },
     "user_tz": -120
    },
    "id": "gF-nNKKfjMar",
    "outputId": "a3388ccc-f068-40c9-cfb1-0cb522422131"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "#file_path = '/content/drive/MyDrive/Tesi_Magistrale'\n",
    "#data_path = '/content/drive/MyDrive/Tesi_Magistrale/DATA/'\n",
    "\n",
    "file_path = 'D:/Tesi_Magistrale'\n",
    "data_path = 'D:/Tesi_Magistrale/DATA/'\n",
    "\n",
    "subjects = ['Sub1','Sub2','Sub3']\n",
    "#subj = 'Manfredo'\n",
    "day = 1\n",
    "single_model = False\n",
    "\n",
    "# constants\n",
    "WND_LEN = 200\n",
    "NORM_MODE = 'sub'\n",
    "NORM_TYPE = 'zero_to_one'\n",
    "RECT = True\n",
    "split_mode = 'rep'\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "exe_labels = ['medium_wrap', 'lateral', 'power_sphere', 'power_disk', 'prismatic_pinch', 'index_extension',\n",
    "                    'wave_out', 'wave_in', 'fist', 'open_hand']\n",
    "'''\n",
    "test1\n",
    "[[[1,2,3,4],[5],[6]],\n",
    " [[1,2,3],[4],[5,6]],\n",
    " [[1,2],[3],[4,5,6]],\n",
    " [[1],[2],[3,4,5,6]]]\n",
    "test2\n",
    "[[[1,2,5,6],[3],[4]],\n",
    " [[1,2,5],[3],[4,6]],\n",
    " [[2,5],[3],[1,4,6]],\n",
    " [[2],[3],[1,4,5,6]]]\n",
    "test3\n",
    "[[[2,4,6],[1,5],[3]],\n",
    " [[2,6],[1,5],[3,4]],\n",
    " [[2],[1,5],[3,4,6]]]\n",
    "\n",
    "'''\n",
    "\n",
    "test = 2\n",
    "train_val_test_rep_combo = [[[1,2,5,6],[3],[4]],\n",
    " [[1,2,5],[3],[4,6]],\n",
    " [[2,5],[3],[1,4,6]],\n",
    " [[2],[3],[1,4,5,6]]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LHKYWkdAkTbM",
    "outputId": "155d8c34-cbaa-4571-8c72-48e2e77e79e9",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Multimodel Testing\n",
      "Models Created\n",
      "Weights Loaded\n",
      "minAngle: 0 maxAngle: 115\n",
      "Angle Intervals: [15, 45, 70, 100]\n",
      "Model: 5\tAngle: 115\tPredicted pose: [9]\tReal Pose: [9]\n",
      "Testing Done!\n",
      "Saving Confusion Matrix\n",
      "Conf_Matrix saved in -->  D:/Tesi_Magistrale/DATA//Manfredo_offset_relabel/day1/Prova_3/Rep_Tests/test2/2_3_1456/Cross_rep/Conf_Matrix/Cross_rep_N_wnd_200_zero_to_one_sub_rect_True_multi_model_day1.png\n"
     ]
    }
   ],
   "source": [
    "for trial in range (3,4):\n",
    "    for subj in subjects:\n",
    "        df = pd.read_csv(data_path+f'{subj}_offset_relabel/day{day}/Dataframe/dataframe_wnd_{WND_LEN}_angle_merged.csv')\n",
    "        df = Norm_each_sub_by_own_param(df, mode= NORM_MODE, norm_type= NORM_TYPE, rectify=RECT )\n",
    "        for train_val_test_rep in train_val_test_rep_combo:\n",
    "            test_reps = train_val_test_rep[2]\n",
    "            save_dir_name = ''\n",
    "            for l in train_val_test_rep:\n",
    "                for x in l:\n",
    "                  save_dir_name = save_dir_name + f'{x}'\n",
    "                save_dir_name = save_dir_name+'_'\n",
    "            save_dir_name = save_dir_name[0:-1]\n",
    "            save_path = data_path + f'/{subj}_offset_relabel/day{day}/Prova_{trial}/Rep_Tests/test{test}/{save_dir_name}/'\n",
    "            if not os.path.exists(save_path):\n",
    "                os.makedirs(save_path)\n",
    "            print(f\"Using Train:{train_val_test_rep[0]}, Val:{train_val_test_rep[1]}, Test:{train_val_test_rep[2]}\\nSingle Model: {single_model}\")\n",
    "            print(f'Saving in folder \"{save_path}\"\\n')\n",
    "            print(f\"Starting Trial {trial} for sub {subj}\")\n",
    "            \n",
    "            # single model\n",
    "            print(\"Training single model\")\n",
    "            n_channels = [cx for cx in df.columns if 'ch' in cx]\n",
    "            df.loc[:, n_channels] = df.loc[:, n_channels].astype(np.float32)\n",
    "    \n",
    "            df_train, df_val, df_test = train_val_test_split_df(df, mode=split_mode, manual_sel=train_val_test_rep)\n",
    "            train_set = Pandas_Dataset(df_train.groupby('sample_index'))\n",
    "            valid_set = Pandas_Dataset(df_val.groupby('sample_index'))\n",
    "            test_set = Pandas_Dataset(df_test.groupby('sample_index'))\n",
    "    \n",
    "            valid_group = np.unique(df_val[split_mode])\n",
    "            test_group = np.unique(df_test[split_mode])\n",
    "    \n",
    "            # Dataloaders\n",
    "            batch_size = 32\n",
    "            num_workers = 0\n",
    "            params = {'batch_size': batch_size,\n",
    "                    'shuffle': True,\n",
    "                    # 'sampler': sampler,\n",
    "                    'num_workers': num_workers,\n",
    "                    'drop_last': False}\n",
    "    \n",
    "            train_set_generator = data.DataLoader(train_set, **params)\n",
    "            valid_set_generator = data.DataLoader(valid_set, **params)\n",
    "            test_set_generator = data.DataLoader(test_set, **params)\n",
    "    \n",
    "            # Model\n",
    "            kernels_gap = [g for g in range(0, 3 * round(WND_LEN / 20), round(WND_LEN / 20))]\n",
    "            kernel_sizes = np.full((3, 5, 2), [1, 3])\n",
    "            for j in range(3):\n",
    "                for i in range(5):\n",
    "                    kernel_sizes[j][i][0] = (round(WND_LEN / 20) * (i + 1) + kernels_gap[j])\n",
    "    \n",
    "            n_classes = 10\n",
    "            net = {'N_multik': 32, 'N_Conv_conc': 64, 'N_SepConv': 128,\n",
    "            'Kernel_multi_dim': kernel_sizes[0],\n",
    "            'Kernel_Conv_conc': 1,\n",
    "            'act_func': nn.ReLU(),\n",
    "            'Pool_Type': nn.MaxPool2d,\n",
    "            'wnd_len':WND_LEN\n",
    "            }\n",
    "    \n",
    "            model = MKCNN_grid(net, num_classes=n_classes)\n",
    "            model = model.to(device)\n",
    "    \n",
    "            #%% Loss Optim and scheduler\n",
    "            # Define Loss functions\n",
    "            cross_entropy_loss = nn.CrossEntropyLoss(reduction='mean').to(device)\n",
    "            # triplet = nn.TripletMarginLoss(reduction='mean', margin=1, p=2)\n",
    "    \n",
    "            # Define Optimizer\n",
    "            learning_rate = 0.0001\n",
    "            # changed beta values from (0.5,0.999) to (0.9,0.999)\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999), weight_decay=1e-4)\n",
    "    \n",
    "            # # Define Scheduler\n",
    "            precision = 1e-6\n",
    "            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, factor=.2,\n",
    "                                                                patience=5, verbose=True, eps=precision)\n",
    "    \n",
    "            print(f'Model created with Num params: {sum([p.numel() for p in model.parameters() if p.requires_grad])}')\n",
    "    \n",
    "            # Start Training\n",
    "            best_weights, tr_losses, val_losses = train_model_standard(model=model, loss_fun=cross_entropy_loss,\n",
    "                                                                optimizer=optimizer, scheduler=scheduler,\n",
    "                                                                dataloaders={\"train\": train_set_generator,\n",
    "                                                                                \"val\": valid_set_generator},\n",
    "                                                                num_epochs=50, precision=precision,\n",
    "                                                                patience=10, patience_increase=5)\n",
    "    \n",
    "            #%% Saving\n",
    "            database = save_path\n",
    "            filename = f'wnd_{WND_LEN}_{NORM_TYPE}_{NORM_MODE}_rect_{RECT}_single_model_day{day}'\n",
    "            # Save state dict of the model\n",
    "            if not os.path.exists(database + f'Cross_{split_mode}/Best_States/'):\n",
    "                os.makedirs(database + f'Cross_{split_mode}/Best_States/')\n",
    "            torch.save(best_weights['state_dict'],\n",
    "                    database + f'Cross_{split_mode}/Best_States/state_dict_{filename}.pth')\n",
    "    \n",
    "            # %% PlotLoss\n",
    "            if not os.path.exists(database + f'Cross_{split_mode}/Plot/'):\n",
    "                os.makedirs(database + f'Cross_{split_mode}/Plot/')\n",
    "    \n",
    "            PlotLoss(tr_losses, val_loss=val_losses,\n",
    "                    title=f'Cross-{split_mode}',\n",
    "                    path_to_save=database + f'Cross_{split_mode}/Plot/',\n",
    "                    filename=f'Cross_{split_mode}_{filename}.png')\n",
    "    \n",
    "            # %% Conf Matrix\n",
    "            if not os.path.exists(database + f'Cross_{split_mode}/Conf_Matrix/'):\n",
    "                os.makedirs(database + f'Cross_{split_mode}/Conf_Matrix/')\n",
    "            clear_output()\n",
    "            # Evaluation\n",
    "            softmax_block = nn.Softmax(dim=1)\n",
    "            y_true = []\n",
    "            y_pred = []\n",
    "    \n",
    "            test_dataloader = test_set_generator\n",
    "            net = model\n",
    "            # Load Weights\n",
    "            net.load_state_dict(torch.load(database + f'Cross_{split_mode}/Best_States/state_dict_{filename}.pth'))\n",
    "    \n",
    "            net.eval()\n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in test_dataloader:\n",
    "                    # inputs = torch.swapaxes(inputs, 2, 1)  # -> convert from [10,20] to [20,10]\n",
    "                    inputs = inputs[:, None, :, :]\n",
    "                    inputs = inputs.to(device)\n",
    "                    labels_np = labels.cpu().data.numpy()\n",
    "                    # forward\n",
    "                    outputs, _ = net(inputs)\n",
    "                    outputs_np = softmax_block(outputs)\n",
    "                    outputs_np = outputs_np.cpu().data.numpy()\n",
    "                    outputs_np = np.argmax(outputs_np, axis=1)\n",
    "    \n",
    "                    y_pred = np.append(y_pred, outputs_np)\n",
    "                    y_true = np.append(y_true, labels_np)\n",
    "    \n",
    "                cm = metrics.confusion_matrix(y_true=y_true, y_pred=y_pred)\n",
    "                # Fancy confusion matrix\n",
    "                plot_confusion_matrix(cm, target_names=exe_labels, title=f'Confusion Matrix for {split_mode} {test_group}',\n",
    "                                    path_to_save=database + f'Cross_{split_mode}/Conf_Matrix/Cross_{split_mode}_N_{filename}.png')\n",
    "    \n",
    "            # %% Write in csv cross_sub results\n",
    "            # Building columns\n",
    "            header_net = [f'Tested_{split_mode}', 'Best Val Loss', 'Accuracy', 'Kappa', 'F1_score', 'Best Epoch', 'Norm_Type',\n",
    "                        'Norm_Mode', 'Rect', f'Valid_{split_mode}']\n",
    "    \n",
    "            # Open the CSV file and write the headers and row of values\n",
    "            with open(database + f'Cross_{split_mode}/Evals_single_model.csv', 'a', newline='') as myFile:\n",
    "                writer = csv.writer(myFile)\n",
    "                if myFile.tell() == 0:\n",
    "                    writer.writerow(header_net)\n",
    "                # Create the row of values\n",
    "                row = [test_group, min(val_losses),\n",
    "                    metrics.accuracy_score(y_true=y_true, y_pred=y_pred),\n",
    "                    metrics.cohen_kappa_score(y1=y_true, y2=y_pred, weights='quadratic'),\n",
    "                    metrics.f1_score(y_true=y_true, y_pred=y_pred, average='macro'),\n",
    "                    best_weights['epoch'], NORM_TYPE, NORM_MODE, str(RECT), valid_group]\n",
    "                writer.writerow(row)\n",
    "                print(f'Results Saved in -> {database}/Cross_{split_mode}/Evals.csv')\n",
    "            clear_output()\n",
    "            \n",
    "            \n",
    "            \n",
    "            # multi angle model\n",
    "            for ANGLE in range(1,6):\n",
    "                print(f'Training angle {ANGLE}')\n",
    "                df_train, df_val, df_test = train_val_test_split_df( df[df['angle_index'] == ANGLE] , mode=split_mode, manual_sel=train_val_test_rep)\n",
    "    \n",
    "                # Saving val and test rep used\n",
    "                valid_group = np.unique(df_val[split_mode])\n",
    "                test_group = np.unique(df_test[split_mode])[0]\n",
    "                # %% pandas dataset\n",
    "                # You must pass to the dataloader a groupby dataframe\n",
    "                train_set = Pandas_Dataset(df_train.groupby('sample_index'))\n",
    "                valid_set = Pandas_Dataset(df_val.groupby('sample_index'))\n",
    "                test_set = Pandas_Dataset(df_test.groupby('sample_index'))\n",
    "    \n",
    "                # %% Dataloaders\n",
    "                batch_size = 32\n",
    "                num_workers = 0\n",
    "                params = {'batch_size': batch_size,\n",
    "                        'shuffle': True,\n",
    "                        # 'sampler': sampler,\n",
    "                        'num_workers': num_workers,\n",
    "                        'drop_last': False}\n",
    "    \n",
    "                train_set_generator = data.DataLoader(train_set, **params)\n",
    "                valid_set_generator = data.DataLoader(valid_set, **params)\n",
    "                test_set_generator = data.DataLoader(test_set, **params)\n",
    "    \n",
    "                #%% Parametric model\n",
    "                kernels_gap = [g for g in range(0, 3 * round(WND_LEN / 20), round(WND_LEN / 20))]\n",
    "                kernel_sizes = np.full((3, 5, 2), [1, 3])\n",
    "                for j in range(3):\n",
    "                    for i in range(5):\n",
    "                        kernel_sizes[j][i][0] = (round(WND_LEN / 20) * (i + 1) + kernels_gap[j])\n",
    "    \n",
    "                n_classes = 10\n",
    "    \n",
    "                net = {'N_multik': 32, 'N_Conv_conc': 64, 'N_SepConv': 128,\n",
    "                'Kernel_multi_dim': kernel_sizes[0],\n",
    "                'Kernel_Conv_conc': 1,\n",
    "                'act_func': nn.ReLU(),\n",
    "                'Pool_Type': nn.MaxPool2d,\n",
    "                'wnd_len':WND_LEN\n",
    "                }\n",
    "    \n",
    "                model = MKCNN_grid(net, num_classes=n_classes)\n",
    "                model = model.to(device)\n",
    "    \n",
    "                #%% Loss Optim and scheduler\n",
    "                # Define Loss functions\n",
    "                cross_entropy_loss = nn.CrossEntropyLoss(reduction='mean').to(device)\n",
    "                # triplet = nn.TripletMarginLoss(reduction='mean', margin=1, p=2)\n",
    "        \n",
    "                # Define Optimizer\n",
    "                learning_rate = 0.0001\n",
    "                # changed beta values from (0.5,0.999) to (0.9,0.999)\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999), weight_decay=1e-4)\n",
    "        \n",
    "                # # Define Scheduler\n",
    "                precision = 1e-6\n",
    "                scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, factor=.2,\n",
    "                                                                    patience=5, verbose=True, eps=precision)\n",
    "    \n",
    "                print(f'Model created with Num params: {sum([p.numel() for p in model.parameters() if p.requires_grad])}')\n",
    "    \n",
    "                # Training\n",
    "                best_weights, tr_losses, val_losses = train_model_standard(model=model, loss_fun=cross_entropy_loss,\n",
    "                                                                      optimizer=optimizer, scheduler=scheduler,\n",
    "                                                                      dataloaders={\"train\": train_set_generator,\n",
    "                                                                                      \"val\": valid_set_generator},\n",
    "                                                                      num_epochs=50, precision=precision,\n",
    "                                                                      patience=10, patience_increase=5)\n",
    "                #%% Saving\n",
    "                database_angle = save_path\n",
    "                filename = f'wnd_{WND_LEN}_{NORM_TYPE}_{NORM_MODE}_rect_{RECT}_ang_{ANGLE}_day{day}'\n",
    "    \n",
    "                # Save state dict of the model\n",
    "                if not os.path.exists(database_angle + f'Cross_{split_mode}/Best_States/'):\n",
    "                    os.makedirs(database_angle + f'Cross_{split_mode}/Best_States/')\n",
    "                torch.save(best_weights['state_dict'],\n",
    "                        database_angle + f'Cross_{split_mode}/Best_States/state_dict_{filename}.pth')\n",
    "    \n",
    "                # %% PlotLoss\n",
    "                if not os.path.exists(database_angle + f'Cross_{split_mode}/Plot/'):\n",
    "                    os.makedirs(database_angle + f'Cross_{split_mode}/Plot/')\n",
    "    \n",
    "                PlotLoss(tr_losses, val_loss=val_losses,\n",
    "                        title=f'Cross-{split_mode}',\n",
    "                        path_to_save=database_angle + f'Cross_{split_mode}/Plot/',\n",
    "                        filename=f'Cross_{split_mode}_{filename}.png')\n",
    "    \n",
    "                # %% Conf Matrix\n",
    "                if not os.path.exists(database_angle + f'Cross_{split_mode}/Conf_Matrix/'):\n",
    "                    os.makedirs(database_angle + f'Cross_{split_mode}/Conf_Matrix/')\n",
    "    \n",
    "                # Evaluation\n",
    "                softmax_block = nn.Softmax(dim=1)\n",
    "                y_true = []\n",
    "                y_pred = []\n",
    "    \n",
    "                test_dataloader = test_set_generator\n",
    "                net = model\n",
    "                # Load Weights\n",
    "                net.load_state_dict(torch.load(database_angle + f'Cross_{split_mode}/Best_States/state_dict_{filename}.pth'))\n",
    "    \n",
    "                net.eval()\n",
    "                with torch.no_grad():\n",
    "                    for inputs, labels in test_dataloader:\n",
    "                        # inputs = torch.swapaxes(inputs, 2, 1)  # -> convert from [10,20] to [20,10]\n",
    "                        inputs = inputs[:, None, :, :]\n",
    "                        inputs = inputs.to(device)\n",
    "                        labels_np = labels.cpu().data.numpy()\n",
    "                        # forward\n",
    "                        outputs, _ = net(inputs)\n",
    "                        outputs_np = softmax_block(outputs)\n",
    "                        outputs_np = outputs_np.cpu().data.numpy()\n",
    "                        outputs_np = np.argmax(outputs_np, axis=1)\n",
    "    \n",
    "                        y_pred = np.append(y_pred, outputs_np)\n",
    "                        y_true = np.append(y_true, labels_np)\n",
    "    \n",
    "                    cm = metrics.confusion_matrix(y_true=y_true, y_pred=y_pred)\n",
    "                    # Fancy confusion matrix\n",
    "                    plot_confusion_matrix(cm, target_names=exe_labels, title=f'Confusion Matrix for {split_mode} {test_group}',\n",
    "                                        path_to_save=database_angle + f'Cross_{split_mode}/Conf_Matrix/Cross_{split_mode}_N_{filename}.png')\n",
    "    \n",
    "                # %% Write in csv cross_sub results\n",
    "                # Building columns\n",
    "                header_net = [f'Tested_{split_mode}', 'Best Val Loss', 'Accuracy', 'Kappa', 'F1_score', 'Best Epoch', 'Norm_Type',\n",
    "                            'Norm_Mode', 'Rect', f'Valid_{split_mode}']\n",
    "    \n",
    "                # Open the CSV file and write the headers and row of values\n",
    "                with open(database_angle + f'Cross_{split_mode}/Evals_multi_model_angle_{ANGLE}.csv', 'a', newline='') as myFile:\n",
    "                    writer = csv.writer(myFile)\n",
    "                    if myFile.tell() == 0:\n",
    "                        writer.writerow(header_net)\n",
    "                    # Create the row of values\n",
    "                    row = [test_group, min(val_losses),\n",
    "                        metrics.accuracy_score(y_true=y_true, y_pred=y_pred),\n",
    "                        metrics.cohen_kappa_score(y1=y_true, y2=y_pred, weights='quadratic'),\n",
    "                        metrics.f1_score(y_true=y_true, y_pred=y_pred, average='macro'),\n",
    "                        best_weights['epoch'], NORM_TYPE, NORM_MODE, str(RECT), valid_group]\n",
    "                    writer.writerow(row)\n",
    "                    print(f'Results Saved in -> {database_angle}/Cross_{split_mode}/Evals.csv')\n",
    "                    myFile.close()\n",
    "                clear_output()\n",
    "            net = {'N_multik': 32, 'N_Conv_conc': 64, 'N_SepConv': 128,\n",
    "            'Kernel_multi_dim': kernel_sizes[0],\n",
    "            'Kernel_Conv_conc': 1,\n",
    "            'act_func': nn.ReLU(),\n",
    "            'Pool_Type': nn.MaxPool2d,\n",
    "            'wnd_len':WND_LEN\n",
    "            }\n",
    "            print(\"Starting Multimodel Testing\")\n",
    "            m1 = MKCNN_grid(net, num_classes=n_classes)\n",
    "            m2 = MKCNN_grid(net, num_classes=n_classes)\n",
    "            m3 = MKCNN_grid(net, num_classes=n_classes)\n",
    "            m4 = MKCNN_grid(net, num_classes=n_classes)\n",
    "            m5 = MKCNN_grid(net, num_classes=n_classes)\n",
    "            m1 = m1.to(device)\n",
    "            m2 = m2.to(device)\n",
    "            m3 = m3.to(device)\n",
    "            m4 = m4.to(device)\n",
    "            m5 = m5.to(device)\n",
    "            print(\"Models Created\")\n",
    "            ws = []\n",
    "            for ANGLE in range(1,6):\n",
    "                ws.append(torch.load(save_path + f'Cross_{split_mode}/Best_States/state_dict_wnd_{WND_LEN}_zero_to_one_sub_rect_True_ang_{ANGLE}_day{day}.pth'))\n",
    "            m1.load_state_dict(ws[0])\n",
    "            m2.load_state_dict(ws[1])\n",
    "            m3.load_state_dict(ws[2])\n",
    "            m4.load_state_dict(ws[3])\n",
    "            m5.load_state_dict(ws[4])\n",
    "            print(\"Weights Loaded\")\n",
    "            \n",
    "            # Prepare test_df\n",
    "            test_df = []\n",
    "            for test_rep in test_reps:\n",
    "                test_df.append(df[df['rep'] == test_rep])\n",
    "            if len(test_df)>1:\n",
    "                test_df = pd.concat(test_df)\n",
    "            else:\n",
    "                test_df = test_df[0]\n",
    "\n",
    "            \n",
    "            minAngle, maxAngle = test_df['angle'].abs().min(), test_df['angle'].abs().max()\n",
    "            print(f\"minAngle: {minAngle} maxAngle: {maxAngle}\")\n",
    "            angleRange = maxAngle - minAngle\n",
    "            interval = 5*round(angleRange * 0.125/5) #round interval to the closest multiple of 5\n",
    "            residue = angleRange-interval*8\n",
    "            intervals = [abs(minAngle+interval), abs(minAngle+interval*3), abs(maxAngle-interval*3), abs(maxAngle-interval)]\n",
    "            print(f\"Angle Intervals: {intervals}\")\n",
    "            \n",
    "            test_df = Pandas_Dataset_angle(test_df.groupby('sample_index'))\n",
    "            batch_size = 1\n",
    "            num_workers = 0\n",
    "            params = {'batch_size': batch_size,\n",
    "                        'shuffle': False,\n",
    "                        # 'sampler': sampler,\n",
    "                        'num_workers': num_workers,\n",
    "                        'drop_last': False}\n",
    "            test_df_dataloader = data.DataLoader(test_df, **params)\n",
    "            \n",
    "            m1.eval()\n",
    "            m2.eval()\n",
    "            m3.eval()\n",
    "            m4.eval()\n",
    "            m5.eval()\n",
    "            with torch.no_grad():\n",
    "                y_pred = []\n",
    "                y_true = []\n",
    "                for inputs, labels, angle in test_df_dataloader:\n",
    "                    inputs = inputs[:, None, :, :]\n",
    "                    inputs = inputs.to(device)\n",
    "                    angle = abs(angle.cpu().data.numpy()[0])\n",
    "                    labels_np = labels.cpu().data.numpy()\n",
    "                    if angle<intervals[0]:\n",
    "                        output, _ = m1.forward(inputs)\n",
    "                        model = 1\n",
    "                    elif angle<intervals[1]:\n",
    "                        output, _ = m2.forward(inputs)\n",
    "                        model = 2\n",
    "                    elif angle<intervals[2]:\n",
    "                        output, _ = m3.forward(inputs)\n",
    "                        model = 3\n",
    "                    elif angle<intervals[3]:\n",
    "                        output, _ = m4.forward(inputs)\n",
    "                        model = 4\n",
    "                    else:\n",
    "                        output, _ = m5.forward(inputs)\n",
    "                        model = 5\n",
    "                    outputs_np = softmax_block(output)\n",
    "                    outputs_np = outputs_np.cpu().data.numpy()\n",
    "                    outputs_np = np.argmax(outputs_np, axis=1)\n",
    "                    y_pred = np.append(y_pred, outputs_np)\n",
    "                    y_true = np.append(y_true, labels_np)\n",
    "                    print(f\"Model: {model}\\tAngle: {angle}\\tPredicted pose: {outputs_np}\\tReal Pose: {labels_np}\", end = '\\r')\n",
    "            \n",
    "                print(\"\\nTesting Done!\")\n",
    "            print(\"Saving Confusion Matrix\")\n",
    "            from Data_Processing_Utils import  plot_confusion_matrix\n",
    "            \n",
    "            poses = ['medium_wrap', 'lateral','power_sphere', 'power_disk', 'prismatic_pinch',\n",
    "                        'index_extension', 'wave_out', 'wave_in', 'fist', 'open_hand']\n",
    "            \n",
    "            cm = metrics.confusion_matrix(y_true, y_pred)\n",
    "            title = f\"Confusion Matrix Multi Model {test_rep}\" # title on the confusion matrix\n",
    "            plot_confusion_matrix(cm, target_names=poses, title=f'CM {title} {NORM_TYPE}',\n",
    "                                            path_to_save=save_path + f'Cross_rep/Conf_Matrix/Cross_rep_N_wnd_{WND_LEN}_{NORM_TYPE}_{NORM_MODE}_rect_{RECT}_multi_model_day{day}.png')\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
