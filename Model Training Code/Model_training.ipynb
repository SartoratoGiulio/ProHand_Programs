{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 400
    },
    "executionInfo": {
     "elapsed": 17244,
     "status": "error",
     "timestamp": 1694009491748,
     "user": {
      "displayName": "Giulio Sartorato",
      "userId": "17669452692906763651"
     },
     "user_tz": -120
    },
    "id": "Yy49Dq074LgC",
    "outputId": "18475b7d-1a25-43f0-b32b-92a2395ba3c3"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'scipy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m loguniform\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m loadmat\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m metrics\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'scipy'"
     ]
    }
   ],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "\n",
    "import argparse\n",
    "import csv\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from scipy.stats import loguniform\n",
    "from scipy.io import loadmat\n",
    "from sklearn import metrics\n",
    "from torch.utils import data\n",
    "from IPython.display import clear_output\n",
    "\n",
    "#sys.path.append('/content/drive/MyDrive/Tesi_Magistrale')\n",
    "sys.path.append('D:/Tesi_Magistrale')\n",
    "sys.path.appen('/mnt/sdb2/Tesi_Magistrale')\n",
    "from Data_Processing_Utils import Norm_each_sub_by_own_param,NormalizeData, train_val_test_split_df, PlotLoss, plot_confusion_matrix, \\\n",
    "    Get_Sub_Norm_params, windowing_Dataframe\n",
    "\n",
    "from DATALOADERS import dataframe_dataset_triplet, Pandas_Dataset\n",
    "\n",
    "from MODELS import MKCNN, MKCNN_grid, random_choice, train_model_triplet, pre_train_model_triplet, train_model_standard, MultiKernelConv2D_grid\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom pandas dataset class modified to provide also the angle\n",
    "class Pandas_Dataset_angle(data.Dataset):\n",
    "\n",
    "    def __init__(self, df_grouped_by_samples, return_sub=False):\n",
    "        self.grouped = df_grouped_by_samples\n",
    "        self.channels = [i for i in df_grouped_by_samples.obj.columns if 'ch' in i]\n",
    "        self.indices = list(df_grouped_by_samples.indices)\n",
    "        self.return_sub = return_sub\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.grouped)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        picked_smp = self.grouped.get_group(self.indices[index])\n",
    "        # Picking only the channels columns from a single sample\n",
    "        sample = torch.tensor(picked_smp.loc[:, self.channels].values).type(torch.float32)\n",
    "        # picking only one label for each sample\n",
    "        label = torch.tensor(picked_smp.loc[:, ['label']].head(1).values[0][0]).type(torch.int8)\n",
    "        angle = torch.tensor(picked_smp.loc[:, ['angle']].head(1).values[0][0]).type(torch.int16)\n",
    "        if self.return_sub:\n",
    "            # picking only one subject\n",
    "            sub = torch.tensor(picked_smp.loc[:, ['sub']].head(1).values[0][0]).type(torch.int8)\n",
    "            return sample, label, angle, sub\n",
    "        else:\n",
    "\n",
    "            # It's missing the part in which I Normalize the data for each subject, or for each subject and channels\n",
    "            # It's not a good idea to do that on the dataloader while producing results\n",
    "            return sample, label, angle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GEGJ1vkiD6Z0"
   },
   "outputs": [],
   "source": [
    "def make_merged_df(dfs_path: str, wnd_list: list, save = True, save_path: str = None):\n",
    "    \"\"\"\n",
    "    dfs_path: folder where the windowed dataframes are stored\n",
    "    wnd_list: list with the name of the windowed dfs to merge\n",
    "    \"\"\"\n",
    "    print(\"Merging Angles Dataframes...\")\n",
    "    #wnd_list = [x for x in sorted(os.listdir(dfs_path)) if name in x]\n",
    "    dfs = []\n",
    "    prev_df_lenghts = 0\n",
    "    i = 0\n",
    "    # loop to merge all the windowed dataframes into one, while changing the sample_index accordingly\n",
    "    for dataframe in wnd_list:\n",
    "        df = pd.read_csv(dfs_path+dataframe)\n",
    "        df['sample_index'] = df['sample_index'] + prev_df_lenghts +1*(i>0)\n",
    "        df\n",
    "        dfs.append(df)\n",
    "        prev_df_lenghts = df['sample_index'].iloc[-1]\n",
    "        i+=1\n",
    "\n",
    "    total_df = pd.concat(dfs, ignore_index = True)\n",
    "    #print(total_df.shape)\n",
    "    total_df_chs = [col for col in total_df.columns if 'Ch' in col]\n",
    "    total_df[total_df_chs] = total_df[total_df_chs].astype(np.uint16)\n",
    "    total_df['angle'] = total_df['angle'].astype(np.int16)\n",
    "\n",
    "    minAngle, maxAngle = total_df['angle'].abs().min(), total_df['angle'].abs().max()\n",
    "    print(f\"minAngle: {minAngle} maxAngle: {maxAngle}\")\n",
    "    angleRange = maxAngle - minAngle\n",
    "    interval = 5*round(angleRange * 0.125/5) #round interval to the closest multiple of 5\n",
    "    residue = angleRange-interval*8\n",
    "    intervals = [abs(minAngle+interval), abs(minAngle+interval*3), abs(maxAngle-interval*3), abs(maxAngle-interval)]\n",
    "    print(f\"Angle Intervals: {intervals}\")\n",
    "\n",
    "    angle_index = []\n",
    "    for ang in total_df['angle']:\n",
    "        ang = abs(ang)\n",
    "        if ang < intervals[0]:\n",
    "            angle_index.append(1)\n",
    "        elif ang < intervals[1]:\n",
    "            angle_index.append(2)\n",
    "        elif ang < intervals[2]:\n",
    "            angle_index.append(3)\n",
    "        elif ang < intervals[3]:\n",
    "            angle_index.append(4)\n",
    "        else:\n",
    "            angle_index.append(5)\n",
    "\n",
    "    if 'Unnamed: 0' in total_df.columns:\n",
    "        total_df.drop('Unnamed: 0', axis = 1, inplace =True)\n",
    "    total_df[\"angle_index\"] = angle_index\n",
    "    total_df[\"angle_index\"] = total_df[\"angle_index\"].astype(np.int8)\n",
    "\n",
    "    if save:\n",
    "        if not os.path.exists(save_path):\n",
    "          os.makedirs(save_path)\n",
    "        save_directory = (dfs_path if save_path == None else save_path) + 'dataframe_wnd_200_angle_merged.csv'\n",
    "        total_df.to_csv(save_directory, index = False)\n",
    "        print(\"\\nMerged angles saved at ----> \"+ save_directory+\"\\n\")\n",
    "    return total_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ML6pBth16Kwd"
   },
   "outputs": [],
   "source": [
    "#file_path = '/content/drive/MyDrive/Tesi_Magistrale'\n",
    "#data_path = '/content/drive/MyDrive/Tesi_Magistrale/DATA'\n",
    "\n",
    "file_path = 'D:/Tesi_Magistrale'\n",
    "data_path = 'D:/Tesi_Magistrale/DATA/'\n",
    "\n",
    "file_path = '/mnt/sdb2/Tesi_Magistrale'\n",
    "data_path = '/mnt/sdb2/Tesi_Magistrale/DATA/'\n",
    "\n",
    "subj = 'Sub1'\n",
    "day = 1\n",
    "database = data_path+f'/{subj}_offset_relabel/day{day}/mat'\n",
    "save_path = data_path+f'/{subj}_offset_relabel/day{day}/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 540,
     "status": "ok",
     "timestamp": 1694003122515,
     "user": {
      "displayName": "Giulio Sartorato",
      "userId": "16312762790553065985"
     },
     "user_tz": -120
    },
    "id": "rZ4r2kYhESRO",
    "outputId": "1af36495-f227-4fc6-b1e5-0f7cff89da09"
   },
   "outputs": [],
   "source": [
    "# Directory and data creation\n",
    "#os.chdir(database)\n",
    "#EMG_data = [x for x in sorted(os.listdir(database)) if subj in x]\n",
    "#print(f'Files found in {database}:')\n",
    "#for file in EMG_data:\n",
    "#    print(file)\n",
    "WND_LEN = 200\n",
    "n_channels = 8\n",
    "columns = [f'Ch{ch}' for ch in range(1, n_channels+1)]\n",
    "\n",
    "operate_on_df = False # True if you want to generate the dfs\n",
    "make_dataframes = False\n",
    "merge_days = False\n",
    "merge_angles = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "uIWrXreXEbRN"
   },
   "outputs": [],
   "source": [
    "# Dataframe creation and manipulation\n",
    "if operate_on_df:\n",
    "    if make_dataframes:\n",
    "        for name in EMG_data:\n",
    "            angle = re.search(r'%s(\\d+)' % 'angle_', name).group(1)\n",
    "            mat = loadmat(name)\n",
    "            df = pd.DataFrame(data=mat['emg'], columns=columns, dtype=np.uint32)\n",
    "            df['Exe_Num'] = mat['relabel_stimulus'].astype(np.uint8)\n",
    "            df['Rep_Num'] = mat['relabel_repetition'].astype(np.uint8)\n",
    "            df['Sub'] = np.full(shape=len(df), fill_value=1).astype(np.uint8)\n",
    "            df['Angle'] = mat['angle'].astype(np.int16)\n",
    "            df = df.loc[df['Exe_Num'] != 0]     # Deleting rest position\n",
    "            df = df.loc[df['Rep_Num'] != 0]     # Deleting other, if remaining (shouldn't but some in day1 angle 4) resting position (?)\n",
    "            df['Exe_Num'] = df['Exe_Num'] - 1    # Starting label from 0\n",
    "            #df.to_csv(database + f'dataframe_ang_{angle}_no_rest.csv', columns=df.columns)\n",
    "            windowing_Dataframe(df, WND_LEN, overlap_perc=0.5, NinaPro_Database_Num=None, drop_last=False, keep_angle = True,\n",
    "                                path_to_save=save_path + 'Dataframe/', filename=f'dataframe_wnd_{WND_LEN}_angle_{angle}.csv')\n",
    "\n",
    "    # % Merging day1 and day2, saving day-type in 'sub' column because, being all the signals from same subject (Giulio Sartorato),\n",
    "    # there is no need to keep sub column. Moreover, having some functions working for domain adaptation between subject,\n",
    "    # using column subject to perform INTER-DAY analysis will become easier.\n",
    "    if merge_days:\n",
    "        print(f'Merging: -> dataframe_wnd_{WND_LEN}_angle_merged  of day1 & day2')\n",
    "        path = f'/content/drive/MyDrive/Tesi_Magistrale/DATA/{subj}_offset_relabel/day1/Dataframe/'\n",
    "        os.chdir(path)\n",
    "        df1 = pd.read_csv(path + f'dataframe_wnd_{WND_LEN}_angle_merged.csv')\n",
    "        path = f'/content/drive/MyDrive/Tesi_Magistrale/DATA/{subj}_offset_relabel/day2/Dataframe/'\n",
    "        os.chdir(path)\n",
    "        df2 = pd.read_csv(path + f'dataframe_wnd_{WND_LEN}_angle_merged.csv')\n",
    "\n",
    "        df2['sub'] = df2['sub'] + 1   # It will represent day2\n",
    "        df2['sample_index'] = df2['sample_index'] + max(df1.sample_index) + 1   # Correcting sample_index to be unique for each window\n",
    "\n",
    "        df = pd.concat([df1, df2])\n",
    "        merge_save_dir = f'/content/drive/MyDrive/Tesi_Magistrale/DATA/{subj}_offset_relabel/merged_day1_2/'\n",
    "        if not os.path.exists(merge_save_dir):\n",
    "            os.makedirs(merge_save_dir)\n",
    "        df.to_csv(merge_save_dir + f'Dataframe/df_merged_wnd_{WND_LEN}_ang_merged.csv', columns=df.columns, index=False)\n",
    "\n",
    "    if merge_angles:\n",
    "        save_path = data_path+f'/{subj}_offset_relabel/day{day}/'\n",
    "        s_string = \"_angle\"\n",
    "        df_list = [x for x in os.listdir(save_path+\"/Dataframe/\") if s_string in x]\n",
    "        print(df_list)\n",
    "        make_merged_df(dfs_path=save_path+\"/Dataframe/\",wnd_list=  df_list, save = True, save_path=save_path+\"/Dataframe/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 329,
     "status": "ok",
     "timestamp": 1694003134510,
     "user": {
      "displayName": "Giulio Sartorato",
      "userId": "16312762790553065985"
     },
     "user_tz": -120
    },
    "id": "Vx9didELIKNL",
    "outputId": "eb6d6822-fd2c-42da-db2e-17711c319696"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged: True, Single Model: False\n"
     ]
    }
   ],
   "source": [
    "single_model = False # only one model or one model per angle?\n",
    "merged = True # multiple days or single day?\n",
    "NORM_MODE = 'sub'\n",
    "NORM_TYPE = 'zero_to_one'\n",
    "NORM_WND = False\n",
    "RECT = True\n",
    "train_val_test_rep = [[2, 4, 6], [1, 5], [3]]\n",
    "test_rep = train_val_test_rep[2][0]\n",
    "print(f\"Merged: {merged}, Single Model: {single_model}\")\n",
    "num_trials = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1106540,
     "status": "ok",
     "timestamp": 1694004244010,
     "user": {
      "displayName": "Giulio Sartorato",
      "userId": "16312762790553065985"
     },
     "user_tz": -120
    },
    "id": "O2kvIEYqIXne",
    "outputId": "17c08733-8873-4c5d-dd42-6c2b7656b408",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Multimodel Testing\n",
      "Models Created\n",
      "Weights Loaded\n",
      "minAngle: 0 maxAngle: 115\n",
      "Angle Intervals: [15, 45, 70, 100]\n",
      "Model: 5\tAngle: 115\tPredicted pose: [9]\tReal Pose: [9]\n",
      "Testing Done!\n",
      "Saving Confusion Matrix\n",
      "Conf_Matrix saved in -->  D:/Tesi_Magistrale/DATA//Multi_offset_relabel/day1/Prova_3/test_rep3/Cross_rep/Conf_Matrix/Cross_rep_N_wnd_200_zero_to_one_sub_rect_True_multi_model_day1.png\n"
     ]
    }
   ],
   "source": [
    "for trial in range(3, num_trials+1):\n",
    "    save_path = data_path+f'/{subj}_offset_relabel/day{day}/Prova_{trial}/'\n",
    "    print(f\"Starting Trial: {trial} on subject {subj}\")\n",
    "    if merged:\n",
    "        database = data_path + f'/{subj}_offset_relabel/day{day}/'\n",
    "        name = f'Dataframe/multi_sub_day{day}_wnd_{WND_LEN}.csv'\n",
    "    else:\n",
    "        database = data_path + f'/{subj}_offset_relabel/day{day}/'\n",
    "        name = f'Dataframe/dataframe_wnd_{WND_LEN}_angle_merged.csv'\n",
    "    df = pd.read_csv(database + name,\n",
    "                                dtype={'sub': np.int8, 'label': np.int8, 'rep': np.int8, 'sample_index': np.uint32})\n",
    "    print(f\"Loaded dataset from {database + name}\")\n",
    "    n_channels = [cx for cx in df.columns if 'ch' in cx]\n",
    "    df.loc[:, n_channels] = df.loc[:, n_channels].astype(np.float32)\n",
    "    exe_labels = ['medium_wrap', 'lateral', 'power_sphere', 'power_disk', 'prismatic_pinch', 'index_extension',\n",
    "                    'wave_out', 'wave_in', 'fist', 'open_hand']\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    print(\"Normalization process initiated\")\n",
    "    # Only norm each sub\n",
    "    df = Norm_each_sub_by_own_param(df, mode= NORM_MODE, norm_type= NORM_TYPE, rectify=RECT )\n",
    "    print(\"Dataset Normalized\")\n",
    "    split_mode = 'rep'\n",
    "    df_train, df_val, df_test = train_val_test_split_df(df, mode=split_mode, manual_sel=train_val_test_rep)\n",
    "    \n",
    "    # Saving val and test rep used\n",
    "    valid_group = np.unique(df_val[split_mode])\n",
    "    test_group = np.unique(df_test[split_mode])[0]\n",
    "    # %% pandas dataset\n",
    "    # You must pass to the dataloader a groupby dataframe\n",
    "    train_set = Pandas_Dataset(df_train.groupby('sample_index'))\n",
    "    valid_set = Pandas_Dataset(df_val.groupby('sample_index'))\n",
    "    test_set = Pandas_Dataset(df_test.groupby('sample_index'))\n",
    "    df_test.set_index('sample_index', inplace = True)\n",
    "    \n",
    "    # %% Dataloaders\n",
    "    batch_size = 32\n",
    "    num_workers = 0\n",
    "    params = {'batch_size': batch_size,\n",
    "            'shuffle': True,\n",
    "            # 'sampler': sampler,\n",
    "            'num_workers': num_workers,\n",
    "            'drop_last': False}\n",
    "    \n",
    "    train_set_generator = data.DataLoader(train_set, **params)\n",
    "    valid_set_generator = data.DataLoader(valid_set, **params)\n",
    "    test_set_generator = data.DataLoader(test_set, **params)\n",
    "    \n",
    "    #%% Parametric model\n",
    "    kernels_gap = [g for g in range(0, 3 * round(WND_LEN / 20), round(WND_LEN / 20))]\n",
    "    kernel_sizes = np.full((3, 5, 2), [1, 3])\n",
    "    for j in range(3):\n",
    "        for i in range(5):\n",
    "            kernel_sizes[j][i][0] = (round(WND_LEN / 20) * (i + 1) + kernels_gap[j])\n",
    "    \n",
    "    n_classes = 10\n",
    "    \n",
    "    net = {'N_multik': 32, 'N_Conv_conc': 64, 'N_SepConv': 128,\n",
    "    'Kernel_multi_dim': kernel_sizes[0],\n",
    "    'Kernel_Conv_conc': 1,\n",
    "    'act_func': nn.ReLU(),\n",
    "    'Pool_Type': nn.MaxPool2d,\n",
    "    'wnd_len':WND_LEN\n",
    "    }\n",
    "    \n",
    "    model = MKCNN_grid(net, num_classes=n_classes)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    #%% Loss Optim and scheduler\n",
    "    # Define Loss functions\n",
    "    cross_entropy_loss = nn.CrossEntropyLoss(reduction='mean').to(device)\n",
    "    # triplet = nn.TripletMarginLoss(reduction='mean', margin=1, p=2)\n",
    "    \n",
    "    # Define Optimizer\n",
    "    learning_rate = 0.0001\n",
    "    # changed beta values from (0.5,0.999) to (0.9,0.999)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999), weight_decay=1e-4)\n",
    "    \n",
    "    # # Define Scheduler\n",
    "    precision = 1e-6\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, factor=.2,\n",
    "                                                        patience=5, verbose=True, eps=precision)\n",
    "    \n",
    "    print(f'Model created with Num params: {sum([p.numel() for p in model.parameters() if p.requires_grad])}')\n",
    "    \n",
    "    \n",
    "    # %% Traning model\n",
    "    # if it doesn't work check if I left the angle in the Pandas_Dataset class\n",
    "    best_weights, tr_losses, val_losses = train_model_standard(model=model, loss_fun=cross_entropy_loss,\n",
    "                                                            optimizer=optimizer, scheduler=scheduler,\n",
    "                                                            dataloaders={\"train\": train_set_generator,\n",
    "                                                                            \"val\": valid_set_generator},\n",
    "                                                            num_epochs=50, precision=precision,\n",
    "                                                            patience=10, patience_increase=5)\n",
    "    \n",
    "    #%% Saving\n",
    "    database = save_path + f\"test_rep{test_rep}/\"\n",
    "    if merged:\n",
    "        filename = f'wnd_{WND_LEN}_{NORM_TYPE}_{NORM_MODE}_rect_{RECT}_single_model_merged'\n",
    "    else:\n",
    "        filename = f'wnd_{WND_LEN}_{NORM_TYPE}_{NORM_MODE}_rect_{RECT}_single_model_day{day}'\n",
    "    # Save state dict of the model\n",
    "    if not os.path.exists(database + f'Cross_{split_mode}/Best_States/'):\n",
    "        os.makedirs(database + f'Cross_{split_mode}/Best_States/')\n",
    "    torch.save(best_weights['state_dict'],\n",
    "            database + f'Cross_{split_mode}/Best_States/state_dict_{filename}.pth')\n",
    "    \n",
    "    # %% PlotLoss\n",
    "    if not os.path.exists(database + f'Cross_{split_mode}/Plot/'):\n",
    "        os.makedirs(database + f'Cross_{split_mode}/Plot/')\n",
    "    \n",
    "    PlotLoss(tr_losses, val_loss=val_losses,\n",
    "            title=f'Cross-{split_mode}',\n",
    "            path_to_save=database + f'Cross_{split_mode}/Plot/',\n",
    "            filename=f'Cross_{split_mode}_{filename}.png')\n",
    "    \n",
    "    # %% Conf Matrix\n",
    "    if not os.path.exists(database + f'Cross_{split_mode}/Conf_Matrix/'):\n",
    "        os.makedirs(database + f'Cross_{split_mode}/Conf_Matrix/')\n",
    "    \n",
    "    # Evaluation\n",
    "    softmax_block = nn.Softmax(dim=1)\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    test_dataloader = test_set_generator\n",
    "    net = model\n",
    "    # Load Weights\n",
    "    net.load_state_dict(torch.load(database + f'Cross_{split_mode}/Best_States/state_dict_{filename}.pth'))\n",
    "    \n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_dataloader:\n",
    "            # inputs = torch.swapaxes(inputs, 2, 1)  # -> convert from [10,20] to [20,10]\n",
    "            inputs = inputs[:, None, :, :]\n",
    "            inputs = inputs.to(device)\n",
    "            labels_np = labels.cpu().data.numpy()\n",
    "            # forward\n",
    "            outputs, _ = net(inputs)\n",
    "            outputs_np = softmax_block(outputs)\n",
    "            outputs_np = outputs_np.cpu().data.numpy()\n",
    "            outputs_np = np.argmax(outputs_np, axis=1)\n",
    "    \n",
    "            y_pred = np.append(y_pred, outputs_np)\n",
    "            y_true = np.append(y_true, labels_np)\n",
    "    \n",
    "        cm = metrics.confusion_matrix(y_true=y_true, y_pred=y_pred)\n",
    "        # Fancy confusion matrix\n",
    "        plot_confusion_matrix(cm, target_names=exe_labels, title=f'Confusion Matrix for {split_mode} {test_group}',\n",
    "                            path_to_save=database + f'Cross_{split_mode}/Conf_Matrix/Cross_{split_mode}_N_{filename}.png')\n",
    "    \n",
    "    # %% Write in csv cross_sub results\n",
    "    # Building columns\n",
    "    header_net = [f'Tested_{split_mode}', 'Best Val Loss', 'Accuracy', 'Kappa', 'F1_score', 'Best Epoch', 'Norm_Type',\n",
    "                'Norm_Mode', 'Rect', f'Valid_{split_mode}']\n",
    "    \n",
    "    # Open the CSV file and write the headers and row of values\n",
    "    with open(database + f'Cross_{split_mode}/Evals_single_model.csv', 'a', newline='') as myFile:\n",
    "        writer = csv.writer(myFile)\n",
    "        if myFile.tell() == 0:\n",
    "            writer.writerow(header_net)\n",
    "        # Create the row of values\n",
    "        row = [test_group, min(val_losses),\n",
    "            metrics.accuracy_score(y_true=y_true, y_pred=y_pred),\n",
    "            metrics.cohen_kappa_score(y1=y_true, y2=y_pred, weights='quadratic'),\n",
    "            metrics.f1_score(y_true=y_true, y_pred=y_pred, average='macro'),\n",
    "            best_weights['epoch'], NORM_TYPE, NORM_MODE, str(RECT), valid_group]\n",
    "        writer.writerow(row)\n",
    "        print(f'Results Saved in -> {database}/Cross_{split_mode}/Evals.csv')\n",
    "    clear_output()\n",
    "            \n",
    "    \n",
    "    \n",
    "    if merged:\n",
    "        database = data_path + f'/{subj}_offset_relabel/day{day}/'\n",
    "        name = f'Dataframe/multi_sub_day{day}_wnd_{WND_LEN}.csv'\n",
    "    else:\n",
    "        database = data_path + f'/{subj}_offset_relabel/day{day}/'\n",
    "        name = f'Dataframe/dataframe_wnd_{WND_LEN}_angle_merged.csv'\n",
    "    print(database+name)\n",
    "    df_merged = pd.read_csv(database + name, dtype={'sub': np.int8, 'label': np.int8, 'rep': np.int8, 'sample_index': np.uint32})\n",
    "    \n",
    "    print(f\"Loaded dataset from {database + name}\")\n",
    "    #Norm only across each sub\n",
    "    df_merged = Norm_each_sub_by_own_param(df_merged, mode= NORM_MODE, norm_type= NORM_TYPE, rectify=RECT )\n",
    "    print(\"Dataset Normalized\")\n",
    "    for ANGLE in range(1, 6):\n",
    "        print(f\"Training on angle {ANGLE}\")\n",
    "        df = df_merged[df_merged['angle_index'] == ANGLE]\n",
    "        #print(df.head(10))\n",
    "        #print(df.tail(10))\n",
    "        # name merged df: f'df_merged_wnd_{WND_LEN}_ang_{ANGLE}.csv'\n",
    "        # name normal df: f'Dataframe/dataframe_wnd_{WND_LEN}_angle_{ANGLE}.csv'\n",
    "        # Making it lighter\n",
    "        exe_labels = ['medium_wrap', 'lateral', 'power_sphere', 'power_disk', 'prismatic_pinch', 'index_extension',\n",
    "                    'wave_out', 'wave_in', 'fist', 'open_hand']\n",
    "        device = torch.device('cuda')\n",
    "    \n",
    "        #%% Normalization\n",
    "        # print(\"Normalization process initiated\")\n",
    "        # Norm across each angle and each sub\n",
    "        #df = Norm_each_sub_by_own_param(df, mode= NORM_MODE, norm_type= NORM_TYPE, rectify=RECT )\n",
    "    \n",
    "        #%% Split Dataframe\n",
    "        split_mode = 'rep'\n",
    "        df_train, df_val, df_test = train_val_test_split_df(df, mode=split_mode, manual_sel=train_val_test_rep)\n",
    "        # Saving val and test rep used\n",
    "        valid_group = np.unique(df_val[split_mode])\n",
    "        test_group = np.unique(df_test[split_mode])[0]\n",
    "        # %% pandas dataset\n",
    "        # You must pass to the dataloader a groupby dataframe\n",
    "        train_set = Pandas_Dataset(df_train.groupby('sample_index'))\n",
    "        valid_set = Pandas_Dataset(df_val.groupby('sample_index'))\n",
    "        test_set = Pandas_Dataset(df_test.groupby('sample_index'))\n",
    "        \"\"\"\n",
    "        df_test.set_index('sample_index', inplace = True)\n",
    "        print(\"Saving test set...\")\n",
    "        path_to_save = database + f\"test_rep{test_rep}/test_sets/\"\n",
    "        if not os.path.exists(path_to_save):\n",
    "            os.makedirs(path_to_save)\n",
    "        df_test.to_csv(path_to_save + f\"/test_set_angle_{ANGLE}_merged.csv\") if merged else df_test.to_csv(path_to_save + f\"test_set_angle_{ANGLE}_day_{day}.csv\")\n",
    "        print(f\"Test Set saved\")\n",
    "        \"\"\"\n",
    "        # %% Dataloaders\n",
    "        batch_size = 32\n",
    "        num_workers = 0\n",
    "        params = {'batch_size': batch_size,\n",
    "                'shuffle': True,\n",
    "                # 'sampler': sampler,\n",
    "                'num_workers': num_workers,\n",
    "                'drop_last': False}\n",
    "    \n",
    "        train_set_generator = data.DataLoader(train_set, **params)\n",
    "        valid_set_generator = data.DataLoader(valid_set, **params)\n",
    "        test_set_generator = data.DataLoader(test_set, **params)\n",
    "    \n",
    "        #%% Parametric model\n",
    "        kernels_gap = [g for g in range(0, 3 * round(WND_LEN / 20), round(WND_LEN / 20))]\n",
    "        kernel_sizes = np.full((3, 5, 2), [1, 3])\n",
    "        for j in range(3):\n",
    "            for i in range(5):\n",
    "                kernel_sizes[j][i][0] = (round(WND_LEN / 20) * (i + 1) + kernels_gap[j])\n",
    "    \n",
    "        n_classes = 10\n",
    "    \n",
    "        net = {'N_multik': 32, 'N_Conv_conc': 64, 'N_SepConv': 128,\n",
    "        'Kernel_multi_dim': kernel_sizes[0],\n",
    "        'Kernel_Conv_conc': 1,\n",
    "        'act_func': nn.ReLU(),\n",
    "        'Pool_Type': nn.MaxPool2d,\n",
    "        'wnd_len':WND_LEN\n",
    "        }\n",
    "    \n",
    "        model = MKCNN_grid(net, num_classes=n_classes)\n",
    "        model = model.to(device)\n",
    "    \n",
    "        #%% Loss Optim and scheduler\n",
    "        # Define Loss functions\n",
    "        cross_entropy_loss = nn.CrossEntropyLoss(reduction='mean').to(device)\n",
    "        # triplet = nn.TripletMarginLoss(reduction='mean', margin=1, p=2)\n",
    "    \n",
    "        # Define Optimizer\n",
    "        learning_rate = 0.0001\n",
    "        # changed beta values from (0.5,0.999) to (0.9,0.999)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999), weight_decay=1e-4)\n",
    "    \n",
    "        # # Define Scheduler\n",
    "        precision = 1e-6\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, factor=.2,\n",
    "                                                            patience=5, verbose=True, eps=precision)\n",
    "    \n",
    "        print(f'Model created with Num params: {sum([p.numel() for p in model.parameters() if p.requires_grad])}')\n",
    "    \n",
    "    \n",
    "        # %% Traning model\n",
    "        # if it doesn't work check if I left the angle in the Pandas_Dataset class\n",
    "        best_weights, tr_losses, val_losses = train_model_standard(model=model, loss_fun=cross_entropy_loss,\n",
    "                                                                optimizer=optimizer, scheduler=scheduler,\n",
    "                                                                dataloaders={\"train\": train_set_generator,\n",
    "                                                                                \"val\": valid_set_generator},\n",
    "                                                                num_epochs=50, precision=precision,\n",
    "                                                                patience=10, patience_increase=5)\n",
    "    \n",
    "        #%% Saving\n",
    "        database_angle = save_path + f\"test_rep{test_rep}/\"\n",
    "        if merged:\n",
    "            filename = f'wnd_{WND_LEN}_{NORM_TYPE}_{NORM_MODE}_rect_{RECT}_ang_{ANGLE}_merged'\n",
    "        else:\n",
    "            filename = f'wnd_{WND_LEN}_{NORM_TYPE}_{NORM_MODE}_rect_{RECT}_ang_{ANGLE}_day{day}'\n",
    "        # Save state dict of the model\n",
    "        if not os.path.exists(database_angle + f'Cross_{split_mode}/Best_States/'):\n",
    "            os.makedirs(database_angle + f'Cross_{split_mode}/Best_States/')\n",
    "        torch.save(best_weights['state_dict'],\n",
    "                database_angle + f'Cross_{split_mode}/Best_States/state_dict_{filename}.pth')\n",
    "    \n",
    "        # %% PlotLoss\n",
    "        if not os.path.exists(database_angle + f'Cross_{split_mode}/Plot/'):\n",
    "            os.makedirs(database_angle + f'Cross_{split_mode}/Plot/')\n",
    "    \n",
    "        PlotLoss(tr_losses, val_loss=val_losses,\n",
    "                title=f'Cross-{split_mode}',\n",
    "                path_to_save=database_angle + f'Cross_{split_mode}/Plot/',\n",
    "                filename=f'Cross_{split_mode}_{filename}.png')\n",
    "    \n",
    "        # %% Conf Matrix\n",
    "        if not os.path.exists(database_angle + f'Cross_{split_mode}/Conf_Matrix/'):\n",
    "            os.makedirs(database_angle + f'Cross_{split_mode}/Conf_Matrix/')\n",
    "    \n",
    "        # Evaluation\n",
    "        softmax_block = nn.Softmax(dim=1)\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "    \n",
    "        test_dataloader = test_set_generator\n",
    "        net = model\n",
    "        # Load Weights\n",
    "        net.load_state_dict(torch.load(database_angle + f'Cross_{split_mode}/Best_States/state_dict_{filename}.pth'))\n",
    "    \n",
    "        net.eval()\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_dataloader:\n",
    "                # inputs = torch.swapaxes(inputs, 2, 1)  # -> convert from [10,20] to [20,10]\n",
    "                inputs = inputs[:, None, :, :]\n",
    "                inputs = inputs.to(device)\n",
    "                labels_np = labels.cpu().data.numpy()\n",
    "                # forward\n",
    "                outputs, _ = net(inputs)\n",
    "                outputs_np = softmax_block(outputs)\n",
    "                outputs_np = outputs_np.cpu().data.numpy()\n",
    "                outputs_np = np.argmax(outputs_np, axis=1)\n",
    "    \n",
    "                y_pred = np.append(y_pred, outputs_np)\n",
    "                y_true = np.append(y_true, labels_np)\n",
    "    \n",
    "            cm = metrics.confusion_matrix(y_true=y_true, y_pred=y_pred)\n",
    "            # Fancy confusion matrix\n",
    "            plot_confusion_matrix(cm, target_names=exe_labels, title=f'Confusion Matrix for {split_mode} {test_group}',\n",
    "                                path_to_save=database_angle + f'Cross_{split_mode}/Conf_Matrix/Cross_{split_mode}_N_{filename}.png')\n",
    "    \n",
    "        # %% Write in csv cross_sub results\n",
    "        # Building columns\n",
    "        header_net = [f'Tested_{split_mode}', 'Best Val Loss', 'Accuracy', 'Kappa', 'F1_score', 'Best Epoch', 'Norm_Type',\n",
    "                    'Norm_Mode', 'Rect', f'Valid_{split_mode}']\n",
    "    \n",
    "        # Open the CSV file and write the headers and row of values\n",
    "        with open(database_angle + f'Cross_{split_mode}/Evals_multi_model_angle_{ANGLE}.csv', 'a', newline='') as myFile:\n",
    "            writer = csv.writer(myFile)\n",
    "            if myFile.tell() == 0:\n",
    "                writer.writerow(header_net)\n",
    "            # Create the row of values\n",
    "            row = [test_group, min(val_losses),\n",
    "                metrics.accuracy_score(y_true=y_true, y_pred=y_pred),\n",
    "                metrics.cohen_kappa_score(y1=y_true, y2=y_pred, weights='quadratic'),\n",
    "                metrics.f1_score(y_true=y_true, y_pred=y_pred, average='macro'),\n",
    "                best_weights['epoch'], NORM_TYPE, NORM_MODE, str(RECT), valid_group]\n",
    "            writer.writerow(row)\n",
    "            print(f'Results Saved in -> {database_angle}/Cross_{split_mode}/Evals.csv')\n",
    "        clear_output()\n",
    "    \n",
    "    net = {'N_multik': 32, 'N_Conv_conc': 64, 'N_SepConv': 128,\n",
    "    'Kernel_multi_dim': kernel_sizes[0],\n",
    "    'Kernel_Conv_conc': 1,\n",
    "    'act_func': nn.ReLU(),\n",
    "    'Pool_Type': nn.MaxPool2d,\n",
    "    'wnd_len':WND_LEN\n",
    "    }\n",
    "    print(\"Starting Multimodel Testing\")\n",
    "    m1 = MKCNN_grid(net, num_classes=n_classes)\n",
    "    m2 = MKCNN_grid(net, num_classes=n_classes)\n",
    "    m3 = MKCNN_grid(net, num_classes=n_classes)\n",
    "    m4 = MKCNN_grid(net, num_classes=n_classes)\n",
    "    m5 = MKCNN_grid(net, num_classes=n_classes)\n",
    "    m1 = m1.to(device)\n",
    "    m2 = m2.to(device)\n",
    "    m3 = m3.to(device)\n",
    "    m4 = m4.to(device)\n",
    "    m5 = m5.to(device)\n",
    "    print(\"Models Created\")\n",
    "    ws = []\n",
    "    for ANGLE in range(1,6):\n",
    "        if merged:\n",
    "            ws.append(torch.load(save_path + f'test_rep{test_rep}/Cross_{split_mode}/Best_States/state_dict_wnd_{WND_LEN}_zero_to_one_sub_rect_True_ang_{ANGLE}_merged.pth'))\n",
    "        else:\n",
    "            ws.append(torch.load(save_path + f'test_rep{test_rep}/Cross_{split_mode}/Best_States/state_dict_wnd_{WND_LEN}_zero_to_one_sub_rect_True_ang_{ANGLE}_day{day}.pth'))\n",
    "    m1.load_state_dict(ws[0])\n",
    "    m2.load_state_dict(ws[1])\n",
    "    m3.load_state_dict(ws[2])\n",
    "    m4.load_state_dict(ws[3])\n",
    "    m5.load_state_dict(ws[4])\n",
    "    print(\"Weights Loaded\")\n",
    "    \n",
    "    # Prepare test_df\n",
    "    test_df = df_merged[df_merged['rep'] == test_rep]\n",
    "    \n",
    "    minAngle, maxAngle = test_df['angle'].abs().min(), test_df['angle'].abs().max()\n",
    "    print(f\"minAngle: {minAngle} maxAngle: {maxAngle}\")\n",
    "    angleRange = maxAngle - minAngle\n",
    "    interval = 5*round(angleRange * 0.125/5) #round interval to the closest multiple of 5\n",
    "    residue = angleRange-interval*8\n",
    "    intervals = [abs(minAngle+interval), abs(minAngle+interval*3), abs(maxAngle-interval*3), abs(maxAngle-interval)]\n",
    "    print(f\"Angle Intervals: {intervals}\")\n",
    "    \n",
    "    test_df = Pandas_Dataset_angle(test_df.groupby('sample_index'))\n",
    "    batch_size = 1\n",
    "    num_workers = 0\n",
    "    params = {'batch_size': batch_size,\n",
    "                'shuffle': False,\n",
    "                # 'sampler': sampler,\n",
    "                'num_workers': num_workers,\n",
    "                'drop_last': False}\n",
    "    test_df_dataloader = data.DataLoader(test_df, **params)\n",
    "    \n",
    "    m1.eval()\n",
    "    m2.eval()\n",
    "    m3.eval()\n",
    "    m4.eval()\n",
    "    m5.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred = []\n",
    "        y_true = []\n",
    "        for inputs, labels, angle in test_df_dataloader:\n",
    "            inputs = inputs[:, None, :, :]\n",
    "            inputs = inputs.to(device)\n",
    "            angle = abs(angle.cpu().data.numpy()[0])\n",
    "            labels_np = labels.cpu().data.numpy()\n",
    "            if angle<intervals[0]:\n",
    "                output, _ = m1.forward(inputs)\n",
    "                model = 1\n",
    "            elif angle<intervals[1]:\n",
    "                output, _ = m2.forward(inputs)\n",
    "                model = 2\n",
    "            elif angle<intervals[2]:\n",
    "                output, _ = m3.forward(inputs)\n",
    "                model = 3\n",
    "            elif angle<intervals[3]:\n",
    "                output, _ = m4.forward(inputs)\n",
    "                model = 4\n",
    "            else:\n",
    "                output, _ = m5.forward(inputs)\n",
    "                model = 5\n",
    "            outputs_np = softmax_block(output)\n",
    "            outputs_np = outputs_np.cpu().data.numpy()\n",
    "            outputs_np = np.argmax(outputs_np, axis=1)\n",
    "            y_pred = np.append(y_pred, outputs_np)\n",
    "            y_true = np.append(y_true, labels_np)\n",
    "            print(f\"Model: {model}\\tAngle: {angle}\\tPredicted pose: {outputs_np}\\tReal Pose: {labels_np}\", end = '\\r')\n",
    "    \n",
    "        print(\"\\nTesting Done!\")\n",
    "    print(\"Saving Confusion Matrix\")\n",
    "    from Data_Processing_Utils import  plot_confusion_matrix\n",
    "    \n",
    "    poses = ['medium_wrap', 'lateral','power_sphere', 'power_disk', 'prismatic_pinch',\n",
    "                'index_extension', 'wave_out', 'wave_in', 'fist', 'open_hand']\n",
    "    \n",
    "    cm = metrics.confusion_matrix(y_true, y_pred)\n",
    "    title = f\"Confusion Matrix Multi Model {test_rep}\" # title on the confusion matrix\n",
    "    plot_confusion_matrix(cm, target_names=poses, title=f'CM {title} {NORM_TYPE}',\n",
    "                                    path_to_save=save_path + f'test_rep{test_rep}/Cross_rep/Conf_Matrix/Cross_rep_N_wnd_{WND_LEN}_{NORM_TYPE}_{NORM_MODE}_rect_{RECT}_multi_model_day{day}.png')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "for ANGLE in range(1,6):\n",
    "    ws.append(torch.load(save_path + f'test_rep{test_rep}/Cross_{split_mode}/Best_States/state_dict_wnd_{WND_LEN}_zero_to_one_sub_rect_True_ang_{ANGLE}_merged.pth'))\n",
    "m1.load_state_dict(ws[0])\n",
    "m2.load_state_dict(ws[1])\n",
    "m3.load_state_dict(ws[2])\n",
    "m4.load_state_dict(ws[3])\n",
    "m5.load_state_dict(ws[4])\n",
    "print(\"Weights Loaded\")\n",
    "\n",
    "# Prepare test_df\n",
    "test_df = df_merged[df_merged['rep'] == test_rep]\n",
    "\n",
    "minAngle, maxAngle = test_df['angle'].abs().min(), test_df['angle'].abs().max()\n",
    "print(f\"minAngle: {minAngle} maxAngle: {maxAngle}\")\n",
    "angleRange = maxAngle - minAngle\n",
    "interval = 5*round(angleRange * 0.125/5) #round interval to the closest multiple of 5\n",
    "residue = angleRange-interval*8\n",
    "intervals = [abs(minAngle+interval), abs(minAngle+interval*3), abs(maxAngle-interval*3), abs(maxAngle-interval)]\n",
    "print(f\"Angle Intervals: {intervals}\")\n",
    "\n",
    "test_df = Pandas_Dataset_angle(test_df.groupby('sample_index'))\n",
    "batch_size = 1\n",
    "num_workers = 0\n",
    "params = {'batch_size': batch_size,\n",
    "            'shuffle': False,\n",
    "            # 'sampler': sampler,\n",
    "            'num_workers': num_workers,\n",
    "            'drop_last': False}\n",
    "test_df_dataloader = data.DataLoader(test_df, **params)\n",
    "\n",
    "m1.eval()\n",
    "m2.eval()\n",
    "m3.eval()\n",
    "m4.eval()\n",
    "m5.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    for inputs, labels, angle in test_df_dataloader:\n",
    "        inputs = inputs[:, None, :, :]\n",
    "        inputs = inputs.to(device)\n",
    "        angle = abs(angle.cpu().data.numpy()[0])\n",
    "        labels_np = labels.cpu().data.numpy()\n",
    "        if angle<intervals[0]:\n",
    "            output, _ = m1.forward(inputs)\n",
    "            model = 1\n",
    "        elif angle<intervals[1]:\n",
    "            output, _ = m2.forward(inputs)\n",
    "            model = 2\n",
    "        elif angle<intervals[2]:\n",
    "            output, _ = m3.forward(inputs)\n",
    "            model = 3\n",
    "        elif angle<intervals[3]:\n",
    "            output, _ = m4.forward(inputs)\n",
    "            model = 4\n",
    "        else:\n",
    "            output, _ = m5.forward(inputs)\n",
    "            model = 5\n",
    "        outputs_np = softmax_block(output)\n",
    "        outputs_np = outputs_np.cpu().data.numpy()\n",
    "        outputs_np = np.argmax(outputs_np, axis=1)\n",
    "        y_pred = np.append(y_pred, outputs_np)\n",
    "        y_true = np.append(y_true, labels_np)\n",
    "        print(f\"Model: {model}\\tAngle: {angle}\\tPredicted pose: {outputs_np}\\tReal Pose: {labels_np}\", end = '\\r')\n",
    "\n",
    "    print(\"\\nTesting Done!\")\n",
    "print(\"Saving Confusion Matrix\")\n",
    "from Data_Processing_Utils import  plot_confusion_matrix\n",
    "\n",
    "poses = ['medium_wrap', 'lateral','power_sphere', 'power_disk', 'prismatic_pinch',\n",
    "            'index_extension', 'wave_out', 'wave_in', 'fist', 'open_hand']\n",
    "\n",
    "cm = metrics.confusion_matrix(y_true, y_pred)\n",
    "title = f\"Confusion Matrix Multi Model {test_rep}\" # title on the confusion matrix\n",
    "if not os.path.exists(save_path+'Conf_Matrix/'):\n",
    "    os.makedirs(save_path+'Conf_Matrix/')\n",
    "plot_confusion_matrix(cm, target_names=poses, title=f'CM {title} {NORM_TYPE}',\n",
    "                                path_to_save=save_path + f'test_rep{test_rep}/Cross_rep/Conf_Matrix/Cross_rep_N_wnd_{WND_LEN}_{NORM_TYPE}_{NORM_MODE}_rect_{RECT}_multi_model_day{day}.png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
